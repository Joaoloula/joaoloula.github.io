<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="centered" style="margin: 0 auto; width:800px; word-wrap: break-word;">
<p><a href="http://joaoloula.github.io">Home</a></p>
<p>Code for this post can be found <a href="https://github.com/Joaoloula/sparse-face-verification">here</a></p>
<h1 id="introduction">Introduction</h1>
<p>Suppose we want to implement a biometric system in which, given a picture of a person's face taken by a camera, a software determines whether this person belongs to a predefined group of people who are allowed to perform a certain action -- this could be giving someone access to a building, or allowing them to start up a car or unlock a smartphone -- and takes a decision accordingly. One of the possible approaches to the system's design is to have a list of images of authorized users' faces and, when confronted with a new person, to analyze whether the image of their face matches with that of one of these users. This problem is known as <em>face verification</em>, and it's an open question that is the subject of a lot of current research in computer vision.</p>
<p>The diversity of situations described indicate that such a software, in order to have satisfactory performance, should be robust to most variations found in real-world images: different lighting conditions, rotation, misalignment etc.</p>
<p>If, on top of that, we also want to be able to easily add people to the authorized users group, it would be advantageous if our system was able to take the decision described earlier based on <em>sparse</em> data, that is, a small number of example pictures per user in the authorized users group. That way, the process of adding users to the group would be only a matter of taking one or two pictures of their face, which would be then added to the database.</p>
<h1 id="face-verification">Face Verification</h1>
<p>Face verification can be thought of as a classification problem: given a face image space <span class="math inline">\(E\)</span>, we are trying to determine a function <span class="math inline">\(f:E\times E \rightarrow \{0, 1\}\)</span> that associates the pair <span class="math inline">\((x_1, x_2)\)</span> to <span class="math inline">\(0\)</span> if it is a genuine pair (i.e. if they represent the same person) and to <span class="math inline">\(1\)</span> if it is an impostor pair (i.e. if they represent images of different people).</p>


<center>
<figure>
<img src="https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/genuine-impostor.jpg"; align='middle'/>
<figcaption>
<sup> Example of genuine (top) and impostor (bottom) pairs from the LFW dataset (source: <span class="citation">(KÃ¶stinger et al. 2012)</span>).</sup></figcaption></code></pre>
</figure>
</center>

<p>Framed in the domain of machine learning, the problems becomes learning the function <span class="math inline">\(f\)</span> from a labeled dataset comprised of genuine and impostor pairs. We'll introduce three different approaches to this problem: a simple, linear model of Exemplar Support Vector Machines (SVMs), Siamese Convolutional Neural Networks (CNNs) and the state-of-the-art identification algorithm DeepID.</p>
<h1 id="exemplar-svms">Exemplar SVMs</h1>
<p>The simplest of the three approaches is based on a method introduced by <span class="citation">(Malisiewicz, Gupta, and Efros 2011)</span>: Exemplar SVMs. The idea is to train one linear SVM classifier, that is, a hyperplane separating our data, for each exemplar in the training set, so that we end up in each case with one positive instance and lots of negatives ones. Surprisingly, this very simple idea works really well, getting results close to the state of the art at the PASCAL VOC object classification dataset at the time of its introduction.</p>
<p>First, we run our training set through a Histogram of Oriented Gradients (HOG) descriptor. HOG descriptors are feature descriptors based on gradient detection: the image is divided into cells, in which all the pixels will &quot;vote&quot; for the preferred gradient by means of an histogram (the weight of each pixel's vote is proportional to the gradient magnitude). The resulting set of histograms is the descriptor, and it's been proven to be robust to many kinds of large-scale transformations and thus widely used in object and human detection <span class="citation">(Dallal and Trigs 2005)</span>.</p>
<p>The next step is to fit a linear SVM model for each positive example in the dataset. These SVMs will take as input only that positive example and the thousands of negative ones, and will try to find the hyperplane that maximizes the margin between them in the HOG feature space. The next step is to bring all these exemplar SVMs together by means of a calibration, in which we rescale our space using the validation set so that the best examples get pulled closer to our positive -- and the worst ones, further apart (without altering their order). From there, given a new image, if we want to know whether it represents a given person, we can compute a compound score for it based on all of the person's exemplar SVMs, and decide on a threshold upon which to make our decision.</p>

</div>
<center>
<figure>
<img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/calibration.jpg" align='center'/>
<figcaption>
<sup> Illustration of the calibration step on an Exemplar SVM <span class="citation">(Malisiewicz, Gupta, and Efros 2011)</span>. </sup>
</figcaption>
</figure>
</center>

<div id="centered" style="margin: 0 auto; width:800px; word-wrap: break-word;">
<h1 id="siamese-cnns">Siamese CNNs</h1>
<p>One of the ways to tackle the face verification problem is to search for a distance function such that intrapersonal distances (distances between different photos of the same person) are small and interpersonal distances (distances between photos of different people) are large. In the linear case, this is equivalent to finding a symmetric positive definite matrix <span class="math inline">\({M}\)</span> such that, given <span class="math inline">\({x_1}\)</span> and <span class="math inline">\({x_1}\)</span> :</p>
\begin{equation}
d({x_1}, {x_2}) = \sqrt{{(x_1-x_2)}^T{M}{(x_1-x_2)}}
\end{equation}
<p>satisfies the properties above. This metric is called a Mahalanobis distance, and is has seen wide use in statistics in general and face verification and recognition in particular, specially in combination with Principal Component Analysis as in <span class="citation">(Fan et al. 2013)</span>. The characterization of M allows is to write it as a product of another matrix and its transpose, and so <span class="math inline">\((1)\)</span> is equivalent to:</p>
\begin{equation}
d({x_1}, {x_2}) = ||{W}{x_1}-{W}{x_2}||
\end{equation}
<p>where <span class="math inline">\({M}={W}^T{W}\)</span>.</p>
<p>By the manifold hypothesis, however, face space would have a manifold structure on pixel space, which cannot be adequately captured by linear transformations <span class="citation">(Hu, Lu, and Tan 2014)</span>. One possible solution is to use a neural network to learn a function whose role is analogous to that of <span class="math inline">\({W}\)</span> in the above example, but not restricted to linear maps. This is the option we explore in this section, and to this end we use what's called a Siamese CNNs architecture.</p>
<p>The idea of the Siamese CNNs architecure <span class="citation">(Chopra, Hadsell, and LeCun 2005)</span> is to train two identical CNNs that share parameters, and whose outputs are fed to an energy function that will measure how &quot;dissimilar&quot; they are, upon which we'll then compute our loss function. Gradient descent on this loss propagates to the two CNNs in the same way, preserving the symmetry of the problem.</p>


<center>
<figure>
<img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/siamese-cnns.jpg" align='middle'/>
<figcaption>
<sup> Scheme of the Siamese CNNs architecture (source: <span class="citation">(Chopra, Hadsell, and LeCun 2005)</span>). </sup>
</figcaption>
</figure>
</center>
<p>In our implementation, each CNN is comprised of three convolutions, all of kernel size 6x6, and computing respectively 5, 14 and 60 features, followed by a fully-connected layer that computes 40 features. Convolutions 1 and 2 are also followed by 2x2 max-pooling layers.</p>
<h1 id="deepid">DeepID</h1>
<p>Another way to tackle verification is to think of it as a subproblem of face identification, that is, the classification problem that involves assigning to each person a label: their identity. In the case of face verification, we're just trying to know if this assignment is the same for two given points in our dataset.</p>
<p>The jump from verification to identification can certainly be impractical: in our earlier example of biometrics, for instance, in order to prevent the entrance of undesired people, the owner of the system would ideally have to train his algorithm to recognize all seven billion people on earth. Far from this naive approach, however, lies an interesting connection that makes the exploration of this harder problem worthwhile: both problems are based on the recognition of facial features, so training a neural network to perform the hard problem of identification can in principle give very good descriptors for verification. That is the core idea behind DeepID <span class="citation">(Sun, Xiaogang, and Xiaoou 2014)</span>, a state-of-the-art algorithm for face verification.</p>
<p>DeepID implements a CNN with four convolutional layers, of kernel sizes 4x4, 3x3, 2x2 and 2x2 and computing 20, 40, 60 and 80 features respectively. The first three layers are followed by 2x2 max-pooling, and both convolution 3 and 4 output to a fully-connected layer (named after the algorithm itself) that computes 160 features and will be used for the verification task later. Finally, for the identification task, the final layer is a softmax for classification between all the identities in the dataset.</p>
<figure>
<img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/deepid.jpg" align='middle'/>
<figcaption>
<sup> Visualization of the DeepID architecture (source: <span class="citation">(Sun, Xiaogang, and Xiaoou 2014)</span>). </sup>
</figcaption>
</figure>
<p>After training on the identification task, we can remove the softmax layer and use the fully-connected DeepID layer as a descriptor for an algorithm that will perform verification on a 160-dimensional space. In Sun's paper, the method found to have the best results was the joint-bayesian model.</p>
<p>Joint-bayesian models <span class="citation">(Chen et al. 2012)</span> the class centers <span class="math inline">\(\mu\)</span> as well as the intra-class variations <span class="math inline">\(\epsilon\)</span> both follow a centered gaussian distributions, whose covariance matrices <span class="math inline">\(S_\mu\)</span> and <span class="math inline">\(S_\epsilon\)</span> are the objects we're trying to infer from the data.</p>
\begin{equation}
x = \mu + \epsilon, \; \; \mu\sim\mathcal{N}\big(0, S_\mu), \; \; \epsilon\sim\mathcal{N}\big(0, S_\epsilon)
\end{equation}
<p>Given two observations <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, if we call <span class="math inline">\(H_I\)</span> the hypothesis that they represent the face of the same person and <span class="math inline">\(H_E\)</span> the hypothesis that they come from different people, we can easily see that under <span class="math inline">\(H_I\)</span>, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> share the same class center and have independent intra-class variation, while under <span class="math inline">\(H_E\)</span>, both their class center and intra-class variation are independent. This leads us to the conclusion that the covariance between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> under <span class="math inline">\(H_I\)</span> and <span class="math inline">\(H_E\)</span> are respectively:</p>
\begin{equation}
\Sigma_I = \begin{bmatrix} S_\mu+S_\epsilon &amp; S_\mu\\ S_\mu &amp; S_\mu+S_\epsilon\end{bmatrix}, \; \; \Sigma_E = \begin{bmatrix} S_\mu+S_\epsilon &amp; 0\\ 0 &amp; S_\mu+S_\epsilon\end{bmatrix}
\end{equation}
<p>The covariance matrices are learned jointly through Expectation-Maximization (EM), an algorithm for estimating the maximum likelihood parameter in a latent variable model through iteration of an E-step, in which we compute the distribution of the latent variables using our previous guess for the parameter, and an M-step, in which we update the parameter so as to maximize the joint distribution likelihood (for more on EM, some great notes by Andrew NG can be found <a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">here</a>). The log likelihood here is given by <span class="math inline">\(r(x_1, x_2) = \log{\frac{P(x_1, x_2 | H_I)}{P(x_1, x_2 | H_E)}}\)</span>, and using the equation above we arrive at a closed form for it whose solution can be computed efficiently.</p>
<div id="refs" class="references">
<div id="ref-joint-bayesian">
<p>Chen, D., X. Cao, L. Wang, F. Wen, and Sun. J. 2012. âBayesian Face Revisited: A Joint Formulation.â In <em>Proc. ECCV</em>.</p>
</div>
<div id="ref-siamese-cnns">
<p>Chopra, S., R. Hadsell, and Y. LeCun. 2005. âLearning a Similarity Metric Discriminatively, with Application to Face Verification.â In <em>Proc. CVPR</em>.</p>
</div>
<div id="ref-hog">
<p>Dallal, N., and B. Trigs. 2005. âHistograms of Oriented Gradients for Human Detection.â In <em>Proc. ICCV</em>.</p>
</div>
<div id="ref-mahalanobis">
<p>Fan, Z., M. Ni, M. Sheng, and Z. Wu. 2013. âPrincipal Component Analysis Integrating Mahalanobis Distance for Face Recognition.â In <em>Proc. RVSP</em>.</p>
</div>
<div id="ref-ddml">
<p>Hu, J., J. Lu, and Y.-P. Tan. 2014. âDiscriminative Deep Metric Learning for Face Verification in the Wild.â In <em>Proc. CVPR</em>.</p>
</div>
<div id="title-image">
<p> KÃ¶stinger, M., Hirzer, M., Wohlhart, P. and H. Bischof. 2012. âLarge Scale Metric Learning from Equivalence Constraints.â In <em>Proc. CVPR</em>.</p>
</div>
<div id="ref-exemplar-svm">
<p>Malisiewicz, T., A. Gupta, and A. A. Efros. 2011. âEnsemble of Exemplar-SVMs for Object Detection and Beyond.â In <em>Proc. ICCV</em>.</p>
</div>
<div id="ref-deepid">
<p>Sun, Y., W. Xiaogang, and T. Xiaoou. 2014. âDeep Learning Face Representation from Predicting 10,000 Classes.â In <em>Proc. CVPR</em>.</p>
</div>
</div>
</div>
</body>
</html>
