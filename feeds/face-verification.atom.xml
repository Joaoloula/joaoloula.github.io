<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Waste Books</title><link href="/" rel="alternate"></link><link href="/feeds/face-verification.atom.xml" rel="self"></link><id>/</id><updated>2016-04-17T00:00:00+02:00</updated><entry><title>Sparse Face Verification</title><link href="/face-verification.html" rel="alternate"></link><updated>2016-04-17T00:00:00+02:00</updated><author><name>João Loula</name></author><id>tag:,2016-04-17:face-verification.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Suppose we want to implement a biometric system in which, given a picture of a person's face taken by a camera, a software determines whether this person belongs to a predefined group of people who are allowed to perform a certain action -- this could be giving someone access to a building, or allowing them to start up a car or unlock a smartphone -- and takes a decision accordingly. One of the possible approaches to the system's design is to have a list of images of authorized users' faces and, when confronted with a new person, to analyze whether the image of their face matches with that of one of these users. This problem is known as \textit{face verification}, and it's an open question that is the subject of a lot of current research in computer vision.&lt;/p&gt;
&lt;p&gt;The diversity of situations described indicate that such a software, in order to have satisfactory performance, should be robust to most variations found in real-world images: different lighting conditions, rotation, misalignment etc.&lt;/p&gt;
&lt;p&gt;If, on top of that, we also want to be able to easily add people to the authorized users group, it would be advantageous if our system was able to take the decision described earlier based on \textit{sparse} data, that is, a small number of example pictures per user in the authorized users group. That way, the process of adding users to the group would be only a matter of taking one or two pictures of their face, which would be then added to the database.&lt;/p&gt;
&lt;h1&gt;Face Verification&lt;/h1&gt;
&lt;p&gt;Face verification can be thought of as a classification problem: given a face image space &lt;span class="math"&gt;\(E\)&lt;/span&gt;, we are trying to determine a function &lt;span class="math"&gt;\(f:E\times E \rightarrow \{0, 1\}\)&lt;/span&gt; that associates the pair &lt;span class="math"&gt;\((x_1, x_2)\)&lt;/span&gt; to &lt;span class="math"&gt;\(0\)&lt;/span&gt; if it is a genuine pair (i.e. if they represent the same person) and to &lt;span class="math"&gt;\(1\)&lt;/span&gt; if it is an impostor pair (i.e. if they represent images of different people).&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/genuine-impostor.jpg" alt='missing' align='middle'/&gt;
    &lt;figcaption&gt;Example of genuine (top) and impostor (bottom) pairs from the LFW dataset.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Framed in the domain of machine learning, the problems becomes learning the function &lt;span class="math"&gt;\(f\)&lt;/span&gt; from a labeled dataset comprised of genuine and impostor pairs. We evaluate three different approaches to this problem: a simple, linear model of Exemplar Support Vector Machines (SVMs), Siamese Convolutional Neural Networks (CNNs)  and the state-of-the-art identification algorithm DeepID.&lt;/p&gt;
&lt;h1&gt;Exemplar SVMs&lt;/h1&gt;
&lt;p&gt;The simplest of the three approaches is based on a method introduced by &lt;a href='#exemplar-svm' id='ref-exemplar-svm-1'&gt;(Malisiewicz et al., 2011)&lt;/a&gt;: Exemplar SVMs. The idea is to train one linear SVM classifier, that is, a hyperplane separating our data, for each exemplar in the training set, so that we end up in each case with one positive instance and lots of negatives ones. Surprisingly, this very simple idea works really well, getting results close to the state of the art at the PASCAL VOC object classification dataset at the time of its introduction.&lt;/p&gt;
&lt;p&gt;First, we run our training set through a Histogram of Oriented Gradients (HOG) descriptor. HOG descriptors are feature descriptors based on gradient detection: the image is divided into cells, in which all the pixels will "vote" for the preferred gradient by means of an histogram (the weight of each pixel's vote is proportional to the gradient magnitude). The resulting set of histograms is the descriptor, and it's been proven to be robust to many kinds of large-scale transformations and thus widely used in object and human detection &lt;a href='#hog' id='ref-hog-1'&gt;(Dallal and Trigs, 2005)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The next step is to fit a linear SVM model for each positive example in the dataset. These SVMs will take as input only that positive example and the thousands of negative ones, and will try to find the hyperplane that maximizes the margin between them in the HOG feature space. The next step is to bring all these exemplar SVMs together by means of a calibration, in which we rescale our space using the validation set so that the best examples get pulled closer to our positive -- and the worst ones, further apart (without altering their order). From there, given a new image, if we want to know whether it represents a given person, we can compute a compound score for it based on all of the person's exemplar SVMs, and decide on a threshold upon which to make our decision.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/calibration.jpg" align='middle'/&gt;
  &lt;figcaption&gt; Illustration of the calibration step on an Exemplar SVM. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1&gt;Siamese CNNs&lt;/h1&gt;
&lt;p&gt;One of the ways to tackle the face verification problem is to search for a distance function such that intrapersonal distances (distances between different photos of the same person) are small and interpersonal distances (distances between photos of different people) are large. In the linear case, this is equivalent to finding a symmetric positive definite matrix &lt;span class="math"&gt;\({M}\)&lt;/span&gt; such that, given &lt;span class="math"&gt;\({x_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\({x_1}\)&lt;/span&gt;  :&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
d({x_1}, {x_2}) = \sqrt{{(x_1-x_2)}^T{M}{(x_1-x_2)}}
\end{equation}&lt;/div&gt;
&lt;p&gt;satisfies the properties above. This metric is called a Mahalanobis distance, and is has seen wide use in statistics in general and face verification and recognition in particular, specially in combination with Principal Component Analysis as in \citep{mahalanobis}. The characterization of M allows is to write it as a product of another matrix and its transpose, and so &lt;span class="math"&gt;\((1)\)&lt;/span&gt; is equivalent to:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
d({x_1}, {x_2}) = ||{W}{x_1}-{W}{x_2}||
\end{equation}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\({M}={W}^T{W}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;By the manifold hypothesis, however, face space would have a manifold structure on pixel space, which cannot be adequately captured by linear transformations &lt;a href='#ddml' id='ref-ddml-1'&gt;(Hu et al., 2014)&lt;/a&gt;. One possible solution is to use a neural network to learn a function whose role is analogous to that of &lt;span class="math"&gt;\({W}\)&lt;/span&gt; in the above example, but not restricted to linear maps. This is the option we explore in this section, and to this end we use what's called a Siamese CNNs architecture.&lt;/p&gt;
&lt;p&gt;The idea of the Siamese CNNs architecure &lt;a href='#siamese-cnns' id='ref-siamese-cnns-1'&gt;(Chopra et al., 2005)&lt;/a&gt; is to train two identical CNNs that share parameters, and whose outputs are fed to an energy function that will measure how "dissimilar" they are, upon which we'll then compute our loss function. Gradient descent on this loss propagates to the two CNNs in the same way, preserving the symmetry of the problem. &lt;/p&gt;
&lt;figure&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/siamese-cnns.jpg" align='middle'/&gt;
  &lt;figcaption&gt; Schema of the Siamese CNNs architecture. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In our implementation, each CNN is comprised of three convolutions, all of kernel size 6x6, and computing respectively 5, 14 and 60 features, followed by a fully-connected layer that computes 40 features. Convolutions 1 and 2 are also followed by 2x2 max-pooling layers.&lt;/p&gt;
&lt;h1&gt;DeepID&lt;/h1&gt;
&lt;p&gt;Another way to tackle verification is to think of it as a subproblem of face identification, that is, the classification problem that involves assigning to each person a label: their identity. In the case of face verification, we're just trying to know if this assignment is the same for two given points in our dataset.&lt;/p&gt;
&lt;p&gt;The jump from verification to identification can certainly be impractical: in our earlier example of biometrics, for instance, in order to prevent the entrance of undesired people, the owner of the system would ideally have to train his algorithm to recognize all seven billion people on earth. Far from this naive approach, however, lies an interesting connection that makes the exploration of this harder problem worthwhile: both problems are based on the recognition of facial features, so training a neural network to perform the hard problem of identification can in principle give very good descriptors for verification. That is the core idea behind DeepID &lt;a href='#deepid' id='ref-deepid-1'&gt;(Sun et al., 2014)&lt;/a&gt;, a state-of-the-art algorithm for face verification.&lt;/p&gt;
&lt;p&gt;DeepID implements a CNN with four convolutional layers, of kernel sizes 4x4, 3x3, 2x2 and 2x2 and computing 20, 40, 60 and 80 features respectively. The first three layers are followed by 2x2 max-pooling, and both convolution 3 and 4 output to a fully-connected layer (named after the algorithm itself) that computes 160 features and will be used for the verification task later. Finally, for the identification task, the final layer is a softmax for classification between all the identities in the dataset.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/deepid.jpg" align='middle'/&gt;
  &lt;figcaption&gt; Visualization of the DeepID architecture. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;After training on the identification task, we can remove the softmax layer and use the fully-connected DeepID layer as a descriptor for an algorithm that will perform verification on a 160-dimensional space. In Sun's paper, the method found to have the best results was the joint-bayesian model.&lt;/p&gt;
&lt;p&gt;Joint-bayesian models \citep{joint-bayesian} the class centers &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; as well as the intra-class variations &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; both follow a centered gaussian distributions, whose covariance matrices &lt;span class="math"&gt;\(S_\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(S_\epsilon\)&lt;/span&gt; are the objects we're trying to infer from the data.&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
x = \mu + \epsilon, \; \; \mu\sim\mathcal{N}\big(0, S_\mu), \; \; \epsilon\sim\mathcal{N}\big(0, S_\epsilon)
\end{equation}&lt;/div&gt;
&lt;p&gt;Given two observations &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt;, if we call &lt;span class="math"&gt;\(H_I\)&lt;/span&gt; the hypothesis that they represent the face of the same person and &lt;span class="math"&gt;\(H_E\)&lt;/span&gt; the hypothesis that they come from different people, we can easily see that under &lt;span class="math"&gt;\(H_I\)&lt;/span&gt;, &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; share the same class center and have independent intra-class variation, while under &lt;span class="math"&gt;\(H_E\)&lt;/span&gt;, both their class center and intra-class variation are independent. This leads us to the conclusion that the covariance between &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; under &lt;span class="math"&gt;\(H_I\)&lt;/span&gt; and &lt;span class="math"&gt;\(H_E\)&lt;/span&gt; are respectively:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\Sigma_I = \begin{bmatrix} S_\mu+S_\epsilon &amp;amp; S_\mu\\ S_\mu &amp;amp; S_\mu+S_\epsilon\end{bmatrix}, \; \; \Sigma_E = \begin{bmatrix} S_\mu+S_\epsilon &amp;amp; 0\\ 0 &amp;amp; S_\mu+S_\epsilon\end{bmatrix}
\end{equation}&lt;/div&gt;
&lt;p&gt;The covariance matrices are learned jointly through Expectation-Maximization (EM), an algorithm for estimating the maximum likelihood parameter in a latent variable model through iteration of an E-step, in which we compute the distribution of the latent variables using our previous guess for the parameter, and an M-step, in which we update the parameter so as to maximize the joint distribution likelihood (for more on EM, some great notes by Andrew NG can be found &lt;a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf"&gt;here&lt;/a&gt;). The log likelihood here is given by &lt;span class="math"&gt;\(r(x_1, x_2) = \log{\frac{P(x_1, x_2 | H_I)}{P(x_1, x_2 | H_E)}}\)&lt;/span&gt;, and using the equation above  we arrive at a closed form for it whose solution can be computed efficiently. &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='siamese-cnns'&gt;S.&amp;nbsp;Chopra, R.&amp;nbsp;Hadsell, and Y.&amp;nbsp;LeCun.
Learning a similarity metric discriminatively, with application to face verification.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2005. &lt;a class="cite-backref" href="#ref-siamese-cnns-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='hog'&gt;N.&amp;nbsp;Dallal and B.&amp;nbsp;Trigs.
Histograms of oriented gradients for human detection.
In &lt;em&gt;Proc. ICCV&lt;/em&gt;. 2005. &lt;a class="cite-backref" href="#ref-hog-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='ddml'&gt;J.&amp;nbsp;Hu, J.&amp;nbsp;Lu, and Y.-P. Tan.
Discriminative deep metric learning for face verification in the wild.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2014. &lt;a class="cite-backref" href="#ref-ddml-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='exemplar-svm'&gt;T.&amp;nbsp;Malisiewicz, A.&amp;nbsp;Gupta, and A.&amp;nbsp;A. Efros.
Ensemble of exemplar-svms for object detection and beyond.
In &lt;em&gt;Proc. ICCV&lt;/em&gt;. 2011. &lt;a class="cite-backref" href="#ref-exemplar-svm-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='deepid'&gt;Y.&amp;nbsp;Sun, W.&amp;nbsp;Xiaogang, and T.&amp;nbsp;Xiaoou.
Deep learning face representation from predicting 10,000 classes.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2014. &lt;a class="cite-backref" href="#ref-deepid-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>