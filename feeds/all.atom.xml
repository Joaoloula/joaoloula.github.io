<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Waste Books</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2016-04-17T00:00:00+02:00</updated><entry><title>Sparse Face Verification</title><link href="/face-verification.html" rel="alternate"></link><updated>2016-04-17T00:00:00+02:00</updated><author><name>Jo√£o Loula</name></author><id>tag:,2016-04-17:face-verification.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Suppose we want to implement a biometric system in which, given a picture of a person's face taken by a camera, a software determines whether this person belongs to a predefined group of people who are allowed to perform a certain action -- this could be giving someone access to a building, or allowing them to start up a car or unlock a smartphone -- and takes a decision accordingly. One of the possible approaches to the system's design is to have a list of images of authorized users' faces and, when confronted with a new person, to analyze whether the image of their face matches with that of one of these users. This problem is known as \textit{face verification}, and it's an open question that is the subject of a lot of current research in computer vision.&lt;/p&gt;
&lt;p&gt;The diversity of situations described indicate that such a software, in order to have satisfactory performance, should be robust to most variations found in real-world images: different lighting conditions, rotation, misalignment etc.&lt;/p&gt;
&lt;p&gt;If, on top of that, we also want to be able to easily add people to the authorized users group, it would be advantageous if our system was able to take the decision described earlier based on \textit{sparse} data, that is, a small number of example pictures per user in the authorized users group. That way, the process of adding users to the group would be only a matter of taking one or two pictures of their face, which would be then added to the database.&lt;/p&gt;
&lt;h1&gt;Face Verification&lt;/h1&gt;
&lt;p&gt;Face verification can be thought of as a classification problem: given a face image space &lt;span class="math"&gt;\(E\)&lt;/span&gt;, we are trying to determine a function &lt;span class="math"&gt;\(f:E\times E \rightarrow \{0, 1\}\)&lt;/span&gt; that associates the pair &lt;span class="math"&gt;\((x_1, x_2)\)&lt;/span&gt; to &lt;span class="math"&gt;\(0\)&lt;/span&gt; if it is a genuine pair (i.e. if they represent the same person) and to &lt;span class="math"&gt;\(1\)&lt;/span&gt; if it is an impostor pair (i.e. if they represent images of different people).&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/genuine-impostor.jpg" alt='missing' align='middle'/&gt;
    &lt;figcaption&gt;Example of genuine (top) and impostor (bottom) pairs from the LFW dataset.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Framed in the domain of machine learning, the problems becomes learning the function &lt;span class="math"&gt;\(f\)&lt;/span&gt; from a labeled dataset comprised of genuine and impostor pairs. We evaluate three different approaches to this problem: a simple, linear model of Exemplar Support Vector Machines (SVMs), Siamese Convolutional Neural Networks (CNNs)  and the state-of-the-art identification algorithm DeepID.&lt;/p&gt;
&lt;h1&gt;Exemplar SVMs&lt;/h1&gt;
&lt;p&gt;The simplest of the three approaches is based on a method introduced by &lt;a href='#exemplar-svm' id='ref-exemplar-svm-1'&gt;(Malisiewicz et al., 2011)&lt;/a&gt;: Exemplar SVMs. The idea is to train one linear SVM classifier, that is, a hyperplane separating our data, for each exemplar in the training set, so that we end up in each case with one positive instance and lots of negatives ones. Surprisingly, this very simple idea works really well, getting results close to the state of the art at the PASCAL VOC object classification dataset at the time of its introduction.&lt;/p&gt;
&lt;p&gt;First, we run our training set through a Histogram of Oriented Gradients (HOG) descriptor. HOG descriptors are feature descriptors based on gradient detection: the image is divided into cells, in which all the pixels will "vote" for the preferred gradient by means of an histogram (the weight of each pixel's vote is proportional to the gradient magnitude). The resulting set of histograms is the descriptor, and it's been proven to be robust to many kinds of large-scale transformations and thus widely used in object and human detection &lt;a href='#hog' id='ref-hog-1'&gt;(Dallal and Trigs, 2005)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The next step is to fit a linear SVM model for each positive example in the dataset. These SVMs will take as input only that positive example and the thousands of negative ones, and will try to find the hyperplane that maximizes the margin between them in the HOG feature space. The next step is to bring all these exemplar SVMs together by means of a calibration, in which we rescale our space using the validation set so that the best examples get pulled closer to our positive -- and the worst ones, further apart (without altering their order). From there, given a new image, if we want to know whether it represents a given person, we can compute a compound score for it based on all of the person's exemplar SVMs, and decide on a threshold upon which to make our decision.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/calibration.jpg" align='middle'/&gt;
  &lt;figcaption&gt; Illustration of the calibration step on an Exemplar SVM. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1&gt;Siamese CNNs&lt;/h1&gt;
&lt;p&gt;One of the ways to tackle the face verification problem is to search for a distance function such that intrapersonal distances (distances between different photos of the same person) are small and interpersonal distances (distances between photos of different people) are large. In the linear case, this is equivalent to finding a symmetric positive definite matrix &lt;span class="math"&gt;\({M}\)&lt;/span&gt; such that, given &lt;span class="math"&gt;\({x_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\({x_1}\)&lt;/span&gt;  :&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
d({x_1}, {x_2}) = \sqrt{{(x_1-x_2)}^T{M}{(x_1-x_2)}}
\end{equation}&lt;/div&gt;
&lt;p&gt;satisfies the properties above. This metric is called a Mahalanobis distance, and is has seen wide use in statistics in general and face verification and recognition in particular, specially in combination with Principal Component Analysis as in \citep{mahalanobis}. The characterization of M allows is to write it as a product of another matrix and its transpose, and so &lt;span class="math"&gt;\((1)\)&lt;/span&gt; is equivalent to:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
d({x_1}, {x_2}) = ||{W}{x_1}-{W}{x_2}||
\end{equation}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\({M}={W}^T{W}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;By the manifold hypothesis, however, face space would have a manifold structure on pixel space, which cannot be adequately captured by linear transformations &lt;a href='#ddml' id='ref-ddml-1'&gt;(Hu et al., 2014)&lt;/a&gt;. One possible solution is to use a neural network to learn a function whose role is analogous to that of &lt;span class="math"&gt;\({W}\)&lt;/span&gt; in the above example, but not restricted to linear maps. This is the option we explore in this section, and to this end we use what's called a Siamese CNNs architecture.&lt;/p&gt;
&lt;p&gt;The idea of the Siamese CNNs architecure &lt;a href='#siamese-cnns' id='ref-siamese-cnns-1'&gt;(Chopra et al., 2005)&lt;/a&gt; is to train two identical CNNs that share parameters, and whose outputs are fed to an energy function that will measure how "dissimilar" they are, upon which we'll then compute our loss function. Gradient descent on this loss propagates to the two CNNs in the same way, preserving the symmetry of the problem. &lt;/p&gt;
&lt;figure&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/siamese-cnns.jpg" align='middle'/&gt;
  &lt;figcaption&gt; Schema of the Siamese CNNs architecture. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In our implementation, each CNN is comprised of three convolutions, all of kernel size 6x6, and computing respectively 5, 14 and 60 features, followed by a fully-connected layer that computes 40 features. Convolutions 1 and 2 are also followed by 2x2 max-pooling layers.&lt;/p&gt;
&lt;h1&gt;DeepID&lt;/h1&gt;
&lt;p&gt;Another way to tackle verification is to think of it as a subproblem of face identification, that is, the classification problem that involves assigning to each person a label: their identity. In the case of face verification, we're just trying to know if this assignment is the same for two given points in our dataset.&lt;/p&gt;
&lt;p&gt;The jump from verification to identification can certainly be impractical: in our earlier example of biometrics, for instance, in order to prevent the entrance of undesired people, the owner of the system would ideally have to train his algorithm to recognize all seven billion people on earth. Far from this naive approach, however, lies an interesting connection that makes the exploration of this harder problem worthwhile: both problems are based on the recognition of facial features, so training a neural network to perform the hard problem of identification can in principle give very good descriptors for verification. That is the core idea behind DeepID &lt;a href='#deepid' id='ref-deepid-1'&gt;(Sun et al., 2014)&lt;/a&gt;, a state-of-the-art algorithm for face verification.&lt;/p&gt;
&lt;p&gt;DeepID implements a CNN with four convolutional layers, of kernel sizes 4x4, 3x3, 2x2 and 2x2 and computing 20, 40, 60 and 80 features respectively. The first three layers are followed by 2x2 max-pooling, and both convolution 3 and 4 output to a fully-connected layer (named after the algorithm itself) that computes 160 features and will be used for the verification task later. Finally, for the identification task, the final layer is a softmax for classification between all the identities in the dataset.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/deepid.jpg" align='middle'/&gt;
  &lt;figcaption&gt; Visualization of the DeepID architecture. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;After training on the identification task, we can remove the softmax layer and use the fully-connected DeepID layer as a descriptor for an algorithm that will perform verification on a 160-dimensional space. In Sun's paper, the method found to have the best results was the joint-bayesian model.&lt;/p&gt;
&lt;p&gt;Joint-bayesian models \citep{joint-bayesian} the class centers &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; as well as the intra-class variations &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; both follow a centered gaussian distributions, whose covariance matrices &lt;span class="math"&gt;\(S_\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(S_\epsilon\)&lt;/span&gt; are the objects we're trying to infer from the data.&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
x = \mu + \epsilon, \; \; \mu\sim\mathcal{N}\big(0, S_\mu), \; \; \epsilon\sim\mathcal{N}\big(0, S_\epsilon)
\end{equation}&lt;/div&gt;
&lt;p&gt;Given two observations &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt;, if we call &lt;span class="math"&gt;\(H_I\)&lt;/span&gt; the hypothesis that they represent the face of the same person and &lt;span class="math"&gt;\(H_E\)&lt;/span&gt; the hypothesis that they come from different people, we can easily see that under &lt;span class="math"&gt;\(H_I\)&lt;/span&gt;, &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; share the same class center and have independent intra-class variation, while under &lt;span class="math"&gt;\(H_E\)&lt;/span&gt;, both their class center and intra-class variation are independent. This leads us to the conclusion that the covariance between &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; under &lt;span class="math"&gt;\(H_I\)&lt;/span&gt; and &lt;span class="math"&gt;\(H_E\)&lt;/span&gt; are respectively:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\Sigma_I = \begin{bmatrix} S_\mu+S_\epsilon &amp;amp; S_\mu\\ S_\mu &amp;amp; S_\mu+S_\epsilon\end{bmatrix}, \; \; \Sigma_E = \begin{bmatrix} S_\mu+S_\epsilon &amp;amp; 0\\ 0 &amp;amp; S_\mu+S_\epsilon\end{bmatrix}
\end{equation}&lt;/div&gt;
&lt;p&gt;The covariance matrices are learned jointly through Expectation-Maximization (EM), an algorithm for estimating the maximum likelihood parameter in a latent variable model through iteration of an E-step, in which we compute the distribution of the latent variables using our previous guess for the parameter, and an M-step, in which we update the parameter so as to maximize the joint distribution likelihood (for more on EM, some great notes by Andrew NG can be found &lt;a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf"&gt;here&lt;/a&gt;). The log likelihood here is given by &lt;span class="math"&gt;\(r(x_1, x_2) = \log{\frac{P(x_1, x_2 | H_I)}{P(x_1, x_2 | H_E)}}\)&lt;/span&gt;, and using the equation above  we arrive at a closed form for it whose solution can be computed efficiently. &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='siamese-cnns'&gt;S.&amp;nbsp;Chopra, R.&amp;nbsp;Hadsell, and Y.&amp;nbsp;LeCun.
Learning a similarity metric discriminatively, with application to face verification.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2005. &lt;a class="cite-backref" href="#ref-siamese-cnns-1" title="Jump back to reference 1"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;p id='hog'&gt;N.&amp;nbsp;Dallal and B.&amp;nbsp;Trigs.
Histograms of oriented gradients for human detection.
In &lt;em&gt;Proc. ICCV&lt;/em&gt;. 2005. &lt;a class="cite-backref" href="#ref-hog-1" title="Jump back to reference 1"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;p id='ddml'&gt;J.&amp;nbsp;Hu, J.&amp;nbsp;Lu, and Y.-P. Tan.
Discriminative deep metric learning for face verification in the wild.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2014. &lt;a class="cite-backref" href="#ref-ddml-1" title="Jump back to reference 1"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;p id='exemplar-svm'&gt;T.&amp;nbsp;Malisiewicz, A.&amp;nbsp;Gupta, and A.&amp;nbsp;A. Efros.
Ensemble of exemplar-svms for object detection and beyond.
In &lt;em&gt;Proc. ICCV&lt;/em&gt;. 2011. &lt;a class="cite-backref" href="#ref-exemplar-svm-1" title="Jump back to reference 1"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
&lt;p id='deepid'&gt;Y.&amp;nbsp;Sun, W.&amp;nbsp;Xiaogang, and T.&amp;nbsp;Xiaoou.
Deep learning face representation from predicting 10,000 classes.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2014. &lt;a class="cite-backref" href="#ref-deepid-1" title="Jump back to reference 1"&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Brownian Motion</title><link href="/brownian-motion.html" rel="alternate"></link><updated>2016-04-02T14:54:00+02:00</updated><author><name>Jo√£o Loula</name></author><id>tag:,2016-04-02:brownian-motion.html</id><summary type="html">&lt;p&gt;In this post, we'll explore different characteristics of the Brownian motion, going from the discrete to the continuous case, and finally expressing the particle's behavior in large times. This model has been the subject of various applications in different areas, like biochemistry, medical imaging and financial markets, and the domain has shown itself to be a fertile subject in mathematics, with important contributions from the likes of Paul Levy, Norbert Wiener and Albert Einstein.&lt;/p&gt;
&lt;p&gt;Code for the simulations can be found &lt;a href="https://github.com/Joaoloula/joaoloula.github.io-src/tree/master/content/posts/brownian-motion/code"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Discovering the Brownian movement&lt;/h2&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(X_i\)&lt;/span&gt; be a sequence of Bernoulli random variables that take the values &lt;span class="math"&gt;\(-1\)&lt;/span&gt; and &lt;span class="math"&gt;\(1\)&lt;/span&gt; with equal probability. We can then define the random variable &lt;span class="math"&gt;\(B_n\)&lt;/span&gt; as: &lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/brown_definition.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;We can show, using the central limit theorem, that &lt;span class="math"&gt;\(B_n\)&lt;/span&gt; converges in law to a centered normal distribution of variance &lt;span class="math"&gt;\(t\)&lt;/span&gt; when &lt;span class="math"&gt;\(n\)&lt;/span&gt; goes to infinity. Taking this limit in a vector &lt;span class="math"&gt;\((B_n(t))\)&lt;/span&gt; whose entries are &lt;span class="math"&gt;\(B_n\)&lt;/span&gt; calculated at increasing times, we obtain the stochastic process &lt;span class="math"&gt;\((B(t))\)&lt;/span&gt;, that we call the Brownian movement. Its first remarkable property is that, by construction, given &lt;span class="math"&gt;\(t\)&lt;/span&gt; bigger than &lt;span class="math"&gt;\(s\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/brown_property.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;It can also be proved that it is continuous but nowhere differentiable. Let's get an idea for what it looks like. In one dimension, for &lt;span class="math"&gt;\(n=100\)&lt;/span&gt; and &lt;span class="math"&gt;\(t=100\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S4-(n=100, t=100).png"/&gt;
&lt;/p&gt;

&lt;p&gt;In two and three dimensions, for &lt;span class="math"&gt;\(n=100\)&lt;/span&gt; and &lt;span class="math"&gt;\(t=1000\)&lt;/span&gt; (the time axis is now ommited):&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S5-2d-(n=100, t=1000).png"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/cover.png"/&gt;
&lt;/p&gt;

&lt;p&gt;Quite the looker, isn't it? We'll now try a more quantitative analysis to get a feel for what's going on.&lt;/p&gt;
&lt;h2&gt;Position and speed distribution in discrete time&lt;/h2&gt;
&lt;p&gt;Robert Brown, the botanist after whom the process is named, first discovered it when observing the movement of pollen particules suspended in water: the random collisions of water mollecules with the particules created a very irregular movement that he was incapable of explaining. The phenomenon was later explained in detail by Einstein in a 1905 paper [1]. The model can be explained as follows: the acceleration of the particule is proportional to the force exercised by the random collisions; there's also a friction force proportional to the particle's speed (we'll call this proportionnality factor lambda). We'll ignore weight as well as buoyancy: we can get away with this because the pollen particle is really small.&lt;/p&gt;
&lt;p&gt;We'll introduce a time interval &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, during which the particle suffers &lt;span class="math"&gt;\(N \alpha\)&lt;/span&gt; collisions (which we suppose to be a very large number). The collisions (which we'll identify by &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt;) are supposed to be independent and identically distributed of random intensity centered at &lt;span class="math"&gt;\(0\)&lt;/span&gt; and square integrable (of variance &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;). We obtain the following expression then for the evolution of the particle's velocity:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/pollen_model.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;Where &lt;span class="math"&gt;\(Y\)&lt;/span&gt; is a random variable of centered gaussian distribution with variance one (which we can infer from applying the central limit theorem to the first expression). To make the notation less cluttered, we'll set &lt;span class="math"&gt;\(a = -\lambda\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(b = \sqrt{N\sigma^2\alpha}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The interest of this formulation is that we can now proceed to quantitaive analysis of the position &lt;span class="math"&gt;\(X_n\)&lt;/span&gt; and velocity &lt;span class="math"&gt;\(V_n\)&lt;/span&gt; of a particle subject to brownian movement (&lt;span class="math"&gt;\(X_n\)&lt;/span&gt; is just a "discrete integration" of the velocity, that is, a &lt;span class="math"&gt;\(\sum\limits_{i} V_i\alpha\)&lt;/span&gt;). &lt;/p&gt;
&lt;p&gt;We can now show that &lt;span class="math"&gt;\(X_n\)&lt;/span&gt; and &lt;span class="math"&gt;\(V_n\)&lt;/span&gt; follow gaussian distributions: even better, by exploiting the independence of  &lt;span class="math"&gt;\((Y_i)\)&lt;/span&gt;, we can prove that any linear combination of &lt;span class="math"&gt;\(X_n\)&lt;/span&gt; and &lt;span class="math"&gt;\(V_n\)&lt;/span&gt; also follows a gaussian distribution, which allows us to conclude that &lt;span class="math"&gt;\((X_n, Y_n)\)&lt;/span&gt; is a gaussian vector. If we note the initial position and velocity by &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(v\)&lt;/span&gt; respectively, its mean is given by:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/xn_vn_mean.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;Now that we're all set up, let's see what happens when we play around with the parameters. This is a simulation for &lt;span class="math"&gt;\(a = 0.5\)&lt;/span&gt; and &lt;span class="math"&gt;\(b = 1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S8-(a=0.5, b=1).png"/&gt;
&lt;/p&gt;

&lt;p&gt;This one, for &lt;span class="math"&gt;\(a = 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(b = 0.1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S8-(a=1, b=0.1).png"/&gt;
&lt;/p&gt;

&lt;p&gt;And finally, for &lt;span class="math"&gt;\(a = 0.1\)&lt;/span&gt; and &lt;span class="math"&gt;\(b = 10\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S8-(a=0.1, b=10).png"/&gt;
&lt;/p&gt;

&lt;p&gt;What we see is that the term &lt;span class="math"&gt;\(a\)&lt;/span&gt; controls the influence of the previous velocity over the next one (the "memory" of the process) and &lt;span class="math"&gt;\(b\)&lt;/span&gt;, it's degree of randomness, which goes well with our characterization of these parameters earlier (multiplying previous velocity and a random gaussian variable in the update rule, respectively). We see, in the first simulation, a random process with an upwards drift, explained by the memory of the initial velocity. In the second one, we have an almost perfect exponential, since the random character of the curve is almost suppressed by the small &lt;span class="math"&gt;\(b\)&lt;/span&gt; value. In the final simulation, however, we have almost no memory and the process is mostly chaotic. This last one is then more likely the one that resembles the most the results that Brown found, and we can use the discrete integration process described earlier to simulate the movement of a pollen particule in the surface of a water tank based on these parameters:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S8-3-pollen2d.png"/&gt;
&lt;/p&gt;

&lt;h2&gt;Continuous time&lt;/h2&gt;
&lt;p&gt;So we've gotten some pretty interesting results so far for our model of Brownian movement, but you might be thinking our formulation is not yet satisfactory for a large range of problems: most systems do not behave in an organized discrete manner, weighing inputs like they were votes and then deciding which action to take at each discrete time step, but are rather complicated phenomena that are at all times being influenced by different factors in a &lt;em&gt;continuous&lt;/em&gt; way -- or at least, like stock markets, chains of events that happen so quickly with respect to our measuring instruments and reaction times that they might as well be continuous. We can then ask ourselves if there's a rigorous mathematical way in which we can expand Brownian motion to continuous time. That's the problem we'll address in this section.&lt;/p&gt;
&lt;p&gt;We'll take &lt;span class="math"&gt;\(K\)&lt;/span&gt; to be the inverse of the timestep &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. We know then, by the previous section's results, that for a fixed &lt;span class="math"&gt;\(K\)&lt;/span&gt;, &lt;span class="math"&gt;\((X_t^K, Y_t^K)\)&lt;/span&gt; a gaussian vector. Their characteristic function &lt;span class="math"&gt;\(\phi_{X_t^K, Y_t^K}\)&lt;/span&gt; is given by:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/phi.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;Where &lt;span class="math"&gt;\(C\)&lt;/span&gt; is the covariance matrix of the position and velocity random variables. We can calculate the limits when &lt;span class="math"&gt;\(K\)&lt;/span&gt; goes to infinity of the two expected values from the formulas we found for the expected values in the previous section:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/vn_limit.jpg"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/xn_limit.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;The limit of the covariance matrix is a little bit more complicated, and so we'll omit it here: it suffices to know that it converges. We then have convergence of the characteristic function to another function &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; that's also the characteristic function of a gaussian. By Lebesgue's dominated convergence theorem, we know that &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; is continuous at zero, and so we can conclude by Levy's theorem that &lt;span class="math"&gt;\((X_t^K, Y_t^K)\)&lt;/span&gt; converges in distribution to a gaussian vector.&lt;/p&gt;
&lt;p&gt;In the continuous limit, velocity variance is well-behaved with respect to time, the variance of the position random variable, however, diverges. To verify this point, we'll plot histograms of &lt;span class="math"&gt;\(X\)&lt;/span&gt; values in the continuous approximation for the times 100, 1000 and 10000, while paying attention to the axes:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S10-1-(t=100).png"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S10-(t=1000).png"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S10-2-(t=10000).png"/&gt;
&lt;/p&gt;

&lt;p&gt;We can se that the positions spread wider and wider as we increase the timescale. &lt;/p&gt;
&lt;h2&gt;A kinetic model&lt;/h2&gt;
&lt;p&gt;This sort of behavior for the position variable, while corresponding to the idea of a free evolution of the particle, is not always desirable. More precisely, in many physical systems, the particle's a priori unbounded movement is counteracted by a force that attracts it to a center (a charged particle, a massive body). This is the sort of model we'll investigate in this section. With this goal in mind, we introduce new velocity and position random variables defined by:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/vn_final.jpg"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/xn_final.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;Where &lt;span class="math"&gt;\(D\)&lt;/span&gt; is called the diffusion coefficient and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; is a strictly positive value.&lt;/p&gt;
&lt;p&gt;The problem with this formulation is that now that the variables are coupled it is quite tricky to solve the system. A more interesting approach would be to do a spectral analysis of the recurrence matrix: for large values of &lt;span class="math"&gt;\(K\)&lt;/span&gt;, the anti-diagonal is dominated by the term &lt;span class="math"&gt;\(-\beta\)&lt;/span&gt;, that plays a regulatory role in the system, and we expect to see imaginary eigenvalues that will define an envelope curve as well as a strong oscillatory behavior. We can run a simulation of the system with large initial conditions in &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(V\)&lt;/span&gt; to verify its behavior:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S13 - (position estabilize jolie).png"/&gt;
&lt;/p&gt;

&lt;p&gt;Besides confirming our hypothesis as we can see from the exponential envelope and sinusoidal oscillations, it is worth noting that the initial conditions become meaningless over time: the system is always brought close to the center of oscillation. What is perhaps most interesting, though, is that the system does not get arbitrarily close to the center, that is, the motion does not converge, as we can see in this simulation with zero initial condition:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S13-(position t=100, K=25).png"/&gt;
&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] http://users.physik.fu-berlin.de/~kleinert/files/eins_brownian.pdf&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="brownian motion"></category><category term="stochastic process"></category><category term="physics"></category></entry></feed>