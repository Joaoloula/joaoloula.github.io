<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>João Loula's Blog</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2016-06-27T00:00:00+02:00</updated><entry><title>Parallel CNN Tracking</title><link href="/parallel-cnn-tracking.html" rel="alternate"></link><updated>2016-06-27T00:00:00+02:00</updated><author><name>João Loula</name></author><id>tag:,2016-06-27:parallel-cnn-tracking.html</id><summary type="html">&lt;p&gt;Code for this post can be found &lt;a href="https://github.com/Joaoloula/siamese-tracking"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The idea of this post is to take the approach described in &lt;a href='#seebymoving' id='ref-seebymoving-1'&gt;(Agrawal et al., 2015)&lt;/a&gt; and implement it in a parallelized fashion. Namely, we will create a Siamese CNNs architecture for object tracking using caffe, and distribute its computations with both coarse and medium-grain parallelization using MPI (for an introduction to neural networks and CNNs, see &lt;a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"&gt;these&lt;/a&gt; &lt;a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular"&gt;two&lt;/a&gt; posts on Christopher Olah's blog, a sketch of the principles behind Siamese CNNs can be found in my &lt;a href="http://joaoloula.github.io/face-verification.html"&gt;face-verification post&lt;/a&gt;. Finally, a great introduction to MPI and High Performance Computing in general is Frank Nielsen's book, whose preview can be found &lt;a href="https://books.google.fr/books?id=eDiFCwAAQBAJ&amp;amp;pg=PR4&amp;amp;lpg=PR4&amp;amp;dq=ecole+polytechnique+hpc+mpi&amp;amp;source=bl&amp;amp;ots=3vsFSyEWs4&amp;amp;sig=wBI83cR9_-u1PNHlE16ryUDrEgw&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwiZ6dCK_cjNAhUCuBoKHfn2CwEQ6AEIIzAB#v=onepage&amp;amp;q=ecole%20polytechnique%20hpc%20mpi&amp;amp;f=false"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;h1&gt;Tracking&lt;/h1&gt;
&lt;p&gt;The goal of this project is to use an architecture called Siamese CNNs to solve an object tracking problem, that is, to map the location of a given object through time in video data, a central problem in areas like autonomous vehicle control and motion-capture videogames.&lt;/p&gt;
&lt;p&gt;Siamese CNNs &lt;a href='#siamese-cnns' id='ref-siamese-cnns-1'&gt;(Chopra et al., 2005)&lt;/a&gt; are a model consisting of two identical CNNs that share all their weights. We can think of them as embedding two inputs into some highly structured space, where this output can then be used by some other function. Notable examples include using Siamese CNNs to determine, given two photos, whether they represent the same person &lt;a href='#deepid2' id='ref-deepid2-1'&gt;(Sun et al., 2014)&lt;/a&gt; or, given two images taking consecutively by a moving vehicle, determine the translational and rotational movements that the vehicle has performed &lt;a href='#seebymoving' id='ref-seebymoving-2'&gt;(Agrawal et al., 2015)&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/siamese-tracking/master/docs/illustrations/siamese-cnns.jpg" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;Visualization of the Siamese CNNs architecture: the two CNNs are identical and share all their weights. In this scheme, their output is directed to an energy function that calculates the norm of the difference (source: &lt;a href='#siamese-cnns' id='ref-siamese-cnns-2'&gt;(Chopra et al., 2005)&lt;/a&gt;).&lt;/sup&gt;
&lt;/figure&gt;

&lt;p&gt;The idea of the implementation is to train the Siamese CNNs model on evenly spaced pairs of frames in a video of an object moving, and to feed their output to another network that will try to learn the object's movement between the two frames.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/siamese-tracking/master/docs/illustrations/trck2.png" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;Example of two video frames to serve as input to the Siamese CNNs model: the bounding box represent the ground-truth of the object movement in the dataset (source: &lt;a href='#car-image' id='ref-car-image-1'&gt;(Mei and Porikli, 2008)&lt;/a&gt;).&lt;/sup&gt;
&lt;/figure&gt;

&lt;h1&gt;A Sip of Caffe&lt;/h1&gt;
&lt;p&gt;Caffe &lt;a href='#caffe' id='ref-caffe-1'&gt;(Jia et al., 2014)&lt;/a&gt; is a deep learning framework written in and interfaced with C++, created by the Berkeley Vision and Learning Center. At its core, it is based on two main objects :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Nets&lt;/em&gt; represent the architecture of the deep neural network : they are comprised of &lt;em&gt;layers&lt;/em&gt; of different types (convolutional, fully-connected, dropout etc.) ;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Blobs&lt;/em&gt; are simply C++ arrays : the data structures being passed along the nets.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Blobs are manipulated throughout the net in &lt;em&gt;forward&lt;/em&gt; and &lt;em&gt;backward&lt;/em&gt; passes : forward passes denote the process in which the neural network takes some data as input and outputs a prediction, while backward passes refer to backpropagation : the comparison of this prediction with the label and the computation of the gradients of the loss function with respect to the parameters throughout the network in a backwards fashion.&lt;/p&gt;
&lt;h1&gt;Models&lt;/h1&gt;
&lt;p&gt;One of the great strengths of Caffe is the fact that its models are stored in plaintext Google Protocol Buffer &lt;a href='#protobuf' id='ref-protobuf-1'&gt;(Google)&lt;/a&gt; schemas : it is highly serializable and human-readable, and interfaces well with many programming languages (such as C++ and Python). Let's take a look at how to declare a convolution layer in protobuf:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;conv1&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Convolution&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;bottom&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;conv1&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;conv1_w&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;lr_mult&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; 
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;conv1_b&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;lr_mult&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; 
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;convolution_param&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;num_output&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;    
    &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;    
    &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;          
    &lt;span class="n"&gt;weight_filler&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gaussian&amp;quot;&lt;/span&gt; 
      &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;        
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;bias_filler&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;constant&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;"Name" and "Type" are very straightforward entries : they define a name and a type for that layer. "Bottom" and "Top" define respectively the input and output of the layer. The "param" section defines rules for the parameters of the layers (weights and biases) : the "name" section will be of utmost importance in this project, since naming the parameters will allow us to share them through networks and thus realize the Siamese CNNs architecture, and "lr_mult" defines the multipliers of the learning rates for the parameters (making the biases change twice as fast as the weights tends to work well in practice).&lt;/p&gt;
&lt;h1&gt;Parallelisation&lt;/h1&gt;
&lt;p&gt;MPI-Caffe &lt;a href='#mpi-caffe' id='ref-mpi-caffe-1'&gt;(Lee et al., 2015)&lt;/a&gt; is a framework built by a group at the University of Indiana to interface MPI with Caffe. By default it parallelizes all layers of the network through all nodes in the cluster : nodes can be included or excluded from computation in specific layers. Communication processes like MPIBroadcast and MPIGather are written as layers in the .protobuf file, and the framework automatically computes the equivalent expression for the gradients in the backward pass.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/siamese-tracking/master/docs/illustrations/mpi_caffe.png" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;Example of a CNN architecture parallelised using MPI-Caffe. The Network is specified on the left, and for each layer there is a "0" when only the root is included in that layer's computation and a "-" when all nodes are included in it. The MPIBroadcast and MPIGather begin and end respectively the parallelised section of the code (source: &lt;a href='#mpi-caffe' id='ref-mpi-caffe-2'&gt;(Lee et al., 2015)&lt;/a&gt;).&lt;/sup&gt;
&lt;/figure&gt;

&lt;p&gt;One of the great advantages of the model is that possibility of parallelisation is twofold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Across Siamese Networks&lt;/em&gt; (medium grain): the calculations performed by each of the two Siamese CNNs can be run independently, with their results being sent back to feed the function on top;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Across Image Pairs&lt;/em&gt; (coarse grain): to increase the number of image pairs in each batch in training, and the speed with which they are processed, we can separate them in mini-batches that are processed across different machines in a cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;MNIST&lt;/h1&gt;
&lt;h2&gt;The Dataset&lt;/h2&gt;
&lt;p&gt;MNIST &lt;a href='#mnist' id='ref-mnist-1'&gt;(LeCun et al., 1999)&lt;/a&gt; is a dataset consisting of 70,000 28x28 grayscale images (split in a train and a test set in a 6:1 proportion) representing handwritten digits, with labels from 0 to 9 that stand for the digit represented by each image. The dataset is stored in the not-so-intuitive IDX file format, but we'll be using &lt;a href="http://pjreddie.com/projects/mnist-in-csv/"&gt;a CSV version available online&lt;/a&gt; in this project.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/siamese-tracking/master/docs/illustrations/mnist.png" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;Example of images from the MNIST dataset (source: Rodrigo Benenson's blog).&lt;/sup&gt;
&lt;/figure&gt;

&lt;h2&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;For the tracking task, preprocessing was done by transforming images in the dataset by a combination of rotations and translations. Rotations were restrained to &lt;span class="math"&gt;\(3°\)&lt;/span&gt; intervals in &lt;span class="math"&gt;\([-30°, 30°]\)&lt;/span&gt;, and translations were chosen as integers in &lt;span class="math"&gt;\([-3, 3]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The task to be learned was posed as classification over the set of possible rotations and translations, with the loss function being the sum of the losses for rotation, x-axis translation and y-axis translation. &lt;/p&gt;
&lt;h2&gt;The Network&lt;/h2&gt;
&lt;p&gt;Using the nomenclature BCNN (for Base Convolutional Neural Network) for the architecture of the Siamese networks and TCNN (for Top Convolutional Neural Network) for the network that takes input from the Siamese CNNs and outputs the final prediction, the architecture used was the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;BCNN :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A convolution layer, with 3x3 kernel and 96 filters, followed by ReLU nonlinearity;&lt;/li&gt;
&lt;li&gt;A 2x2 max-pooling layer;&lt;/li&gt;
&lt;li&gt;A convolution layer, with 3x3 kernel and 256 filters, followed by ReLU;&lt;/li&gt;
&lt;li&gt;A 2x2 max-pooling layer;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TCNN :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A fully-connected layer, with 500 filters, followed by ReLU nonlinearity;&lt;/li&gt;
&lt;li&gt;A dropout layer with 0.5 dropout;&lt;/li&gt;
&lt;li&gt;Three separate fully-connected layers, with 41, 13 and 13 outputs respectively (matching number of rotation, x translation and y translation classes);&lt;/li&gt;
&lt;li&gt;A softmax layer with logistic loss (with equal weights for each of the three predictions).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/siamese-tracking/master/docs/illustrations/bcnn-tcnn.jpg" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;Scheme of a forward pass in the Siamese network: each image in the pair moves through the layers L1, ... Lk in one of the BCNNs, and their output is processed by the TCNN to make the prediction (source: &lt;a href='#seebymoving' id='ref-seebymoving-3'&gt;(Agrawal et al., 2015)&lt;/a&gt;).&lt;/sup&gt;
&lt;/figure&gt;

&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;The network was trained using batches of 64 image pairs, with a base learning rate of &lt;span class="math"&gt;\(10^{-7}\)&lt;/span&gt; and inverse decay with &lt;span class="math"&gt;\(\gamma = 0.1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\text{power}=0.75\)&lt;/span&gt;. The network seemed to converge after about 1000 iterations, to an accuracy of about &lt;span class="math"&gt;\(3\%\)&lt;/span&gt;for the rotation prediction and &lt;span class="math"&gt;\(14\%\)&lt;/span&gt; for the x and y translation predictions (about 1.25 times better than random guessing for the rotation and 2 times better for the translations). &lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/siamese-tracking/master/docs/illustrations/mnist_training.png" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;Value of the loss function throughout training iterations in the model.&lt;/sup&gt;
&lt;/figure&gt;

&lt;h1&gt;Coarse-Grain Parallelization&lt;/h1&gt;
&lt;p&gt;The simplest way to parallelize the program is to run multiple training batches on different nodes, as in the scheme below:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/siamese-tracking/master/docs/illustrations/mpi_caffe_complete.png" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;Example of a CNN architecture using fully parallelised using MPI-Caffe (source: &lt;a href='#mpi-caffe' id='ref-mpi-caffe-3'&gt;(Lee et al., 2015)&lt;/a&gt;)&lt;/sup&gt;.
&lt;/figure&gt;

&lt;p&gt;In this case, we're gaining a speedup in the &lt;a href="https://en.wikipedia.org/wiki/Gustafson%27s_law"&gt;Gustafson sense&lt;/a&gt;, that is, as we raise the number of processors, we also raise the size of the data we can compute in a given time. The speedup expression is then given by:&lt;/p&gt;
&lt;div class="math"&gt;$$\text{speedup}_{\text{Gustafson}}(P) = \alpha_{seq} + P(1 - \alpha_{seq}) $$&lt;/div&gt;
&lt;p&gt;where P is the number of processors and &lt;span class="math"&gt;\(\alpha_{seq}\)&lt;/span&gt; is the proportion of the code that's not being parallelized. Seeing as in this scheme the whole network is being parallelized, we have:&lt;/p&gt;
&lt;div class="math"&gt;$$\alpha_{seq} \approx 0 \Rightarrow \text{speedup}_{\text{Gustafson}}(P) \approx P $$&lt;/div&gt;
&lt;p&gt;Let's see how this fares in practice. In the figure below, we find a comparison of running times for the forward and backward passes in the network for one, two and four cores, the four core option using hyperthreading. What we find is that the two core case follows Gustafson's law closely, with a speedup coefficient of &lt;span class="math"&gt;\(1.93\)&lt;/span&gt;. In the four core case, however, performance is no better than with two cores, which probably means that hyperthreading is making no difference for this task.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/siamese-tracking/master/docs/illustrations/cores_comparison.png" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;Comparison between forward and backward pass times when running the network with 1, 2 or 4 cores with hyperthreading.&lt;/sup&gt;
&lt;/figure&gt;

&lt;h1&gt;Medium-Grain Parallelization&lt;/h1&gt;
&lt;p&gt;The interest of the Siamese CNNs architecture, however, is the possibility of parallelization on a lower level : we can distribute the two BCNN streams to two different nodes in the cluster, and then gather their results to perform the computations on the TCNN. Results are shown in the figure below: we can see that performance is almost as good as in the completely parallelized scheme, which confirms our knowledge that the convolutional layers are by far the most computationally-intensive ones, so that the BCNN accounts for most of the computations in the network. We can also see that the difference between these two parallelization schemes lies almost entirely in the backward pass: we can hypothesize that this is due to increased difficulty in computing the gradient through the gather and broadcast layers in the Medium-Grain scheme. &lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/siamese-tracking/master/docs/illustrations/parallelizations_comparison.png" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;Comparison between forward and backward pass times when running the network with no parallelization, with only the BCNN parallelized or with the whole code parallelized .&lt;/sup&gt;
&lt;/figure&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='seebymoving'&gt;P.&amp;nbsp;Agrawal, J.&amp;nbsp;Carreira, and J.&amp;nbsp;Malik.
Learning to see by moving.
&lt;em&gt;arXiv preprint&lt;/em&gt;, 2015. &lt;a class="cite-backref" href="#ref-seebymoving-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-seebymoving-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-seebymoving-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-seebymoving-3" title="Jump back to reference 3"&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='siamese-cnns'&gt;S.&amp;nbsp;Chopra, R.&amp;nbsp;Hadsell, and Y.&amp;nbsp;LeCun.
Learning a similarity metric discriminatively, with application to face verification.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2005. &lt;a class="cite-backref" href="#ref-siamese-cnns-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-siamese-cnns-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-siamese-cnns-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='protobuf'&gt;Google.
Protocol buffers.
\url http://code.google.com/apis/protocolbuffers/. &lt;a class="cite-backref" href="#ref-protobuf-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='caffe'&gt;Y.&amp;nbsp;Jia, E.&amp;nbsp;Shelhamer, J.&amp;nbsp;Donahue, S.&amp;nbsp;Karayev, J.&amp;nbsp;Long, R.&amp;nbsp;Girshick, S.&amp;nbsp;Guadarrama, and T.&amp;nbsp;Darrell.
Caffe: convolutional architecture for fast feature embedding.
&lt;em&gt;ACM Multimedia Open Source&lt;/em&gt;, 2014. &lt;a class="cite-backref" href="#ref-caffe-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='mnist'&gt;Y.&amp;nbsp;LeCun, C.&amp;nbsp;Cortes, and C.&amp;nbsp;J.C.&amp;nbsp;Burges.
The mnist database of handwritten digits.
&lt;em&gt;http://yann.lecun.com/exdb/mnist&lt;/em&gt;, 1999. &lt;a class="cite-backref" href="#ref-mnist-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='mpi-caffe'&gt;Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David&amp;nbsp;J. Crandall, and Dhruv Batra.
Why M heads are better than one: training a diverse ensemble of deep networks.
&lt;em&gt;arXiv&lt;/em&gt;, 2015.
URL: &lt;a href="http://arxiv.org/abs/1511.06314"&gt;http://arxiv.org/abs/1511.06314&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-mpi-caffe-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-mpi-caffe-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-mpi-caffe-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-mpi-caffe-3" title="Jump back to reference 3"&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='car-image'&gt;X.&amp;nbsp;Mei and F.&amp;nbsp;Porikli.
Joint tracking and video registration by factorial hidden markov models.
In &lt;em&gt;IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)&lt;/em&gt;. 2008. &lt;a class="cite-backref" href="#ref-car-image-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='deepid2'&gt;Y.&amp;nbsp;Sun, X.&amp;nbsp;Wang, and X.&amp;nbsp;Tang.
Deep learning face representation by joint identification-verification.
&lt;em&gt;Proc. NIPS&lt;/em&gt;, 2014. &lt;a class="cite-backref" href="#ref-deepid2-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</summary><category term="tracking"></category><category term="convolutional neural networks"></category><category term="parallel computing"></category></entry><entry><title>Face Verification</title><link href="/face-verification.html" rel="alternate"></link><updated>2016-04-17T00:00:00+02:00</updated><author><name>João Loula</name></author><id>tag:,2016-04-17:face-verification.html</id><summary type="html">&lt;p&gt;Code for this post can be found &lt;a href="https://github.com/Joaoloula/sparse-face-verification"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Suppose we want to implement a biometric system in which, given a picture of a person's face taken by a camera, a software determines whether this person belongs to a predefined group of people who are allowed to perform a certain action -- this could be giving someone access to a building, or allowing them to start up a car or unlock a smartphone -- and takes a decision accordingly. One of the possible approaches to the system's design is to have a list of images of authorized users' faces and, when confronted with a new person, to analyze whether the image of their face matches with that of one of these users. This problem is known as &lt;em&gt;face verification&lt;/em&gt;, and it's an open question that is the subject of a lot of current research in computer vision.&lt;/p&gt;
&lt;p&gt;The diversity of situations described indicate that such a software, in order to have satisfactory performance, should be robust to most variations found in real-world images: different lighting conditions, rotation, misalignment etc.&lt;/p&gt;
&lt;p&gt;If, on top of that, we also want to be able to easily add people to the authorized users group, it would be advantageous if our system was able to take the decision described earlier based on &lt;em&gt;sparse&lt;/em&gt; data, that is, a small number of example pictures per user in the authorized users group. That way, the process of adding users to the group would be only a matter of taking one or two pictures of their face, which would be then added to the database.&lt;/p&gt;
&lt;h1&gt;Face Verification&lt;/h1&gt;
&lt;p&gt;Face verification can be thought of as a classification problem: given a face image space &lt;span class="math"&gt;\(E\)&lt;/span&gt;, we are trying to determine a function &lt;span class="math"&gt;\(f:E\times E \rightarrow \{0, 1\}\)&lt;/span&gt; that associates the pair &lt;span class="math"&gt;\((x_1, x_2)\)&lt;/span&gt; to &lt;span class="math"&gt;\(0\)&lt;/span&gt; if it is a genuine pair (i.e. if they represent the same person) and to &lt;span class="math"&gt;\(1\)&lt;/span&gt; if it is an impostor pair (i.e. if they represent images of different people).&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/genuine-impostor.jpg" alt='missing' align='middle'/&gt;
    &lt;figcaption&gt;&lt;sup&gt; Example of genuine (top) and impostor (bottom) pairs from the LFW dataset (source: &lt;a href='#title-image' id='ref-title-image-1'&gt;(Köstinger et al., 2012)&lt;/a&gt;).&lt;/sup&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Framed in the domain of machine learning, the problems becomes learning the function &lt;span class="math"&gt;\(f\)&lt;/span&gt; from a labeled dataset comprised of genuine and impostor pairs. We'll introduce three different approaches to this problem: a simple, linear model of Exemplar Support Vector Machines (SVMs), Siamese Convolutional Neural Networks (CNNs)  and the state-of-the-art identification algorithm DeepID.&lt;/p&gt;
&lt;h1&gt;Exemplar SVMs&lt;/h1&gt;
&lt;p&gt;The simplest of the three approaches is based on a method introduced by &lt;a href='#exemplar-svm' id='ref-exemplar-svm-1'&gt;(Malisiewicz et al., 2011)&lt;/a&gt;: Exemplar SVMs. The idea is to train one linear SVM classifier, that is, a hyperplane separating our data, for each exemplar in the training set, so that we end up in each case with one positive instance and lots of negatives ones. Surprisingly, this very simple idea works really well, getting results close to the state of the art at the PASCAL VOC object classification dataset at the time of its introduction.&lt;/p&gt;
&lt;p&gt;First, we run our training set through a Histogram of Oriented Gradients (HOG) descriptor. HOG descriptors are feature descriptors based on gradient detection: the image is divided into cells, in which all the pixels will "vote" for the preferred gradient by means of an histogram (the weight of each pixel's vote is proportional to the gradient magnitude). The resulting set of histograms is the descriptor, and it's been proven to be robust to many kinds of large-scale transformations and thus widely used in object and human detection &lt;a href='#hog' id='ref-hog-1'&gt;(Dallal and Trigs, 2005)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The next step is to fit a linear SVM model for each positive example in the dataset. These SVMs will take as input only that positive example and the thousands of negative ones, and will try to find the hyperplane that maximizes the margin between them in the HOG feature space. The next step is to bring all these exemplar SVMs together by means of a calibration, in which we rescale our space using the validation set so that the best examples get pulled closer to our positive -- and the worst ones, further apart (without altering their order). From there, given a new image, if we want to know whether it represents a given person, we can compute a compound score for it based on all of the person's exemplar SVMs, and decide on a threshold upon which to make our decision.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/calibration.jpg" align='middle'/&gt;
  &lt;figcaption&gt;&lt;sup&gt; Illustration of the calibration step on an Exemplar SVM &lt;a href='#exemplar-svm' id='ref-exemplar-svm-2'&gt;(Malisiewicz et al., 2011)&lt;/a&gt;. &lt;/sup&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1&gt;Siamese CNNs&lt;/h1&gt;
&lt;p&gt;One of the ways to tackle the face verification problem is to search for a distance function such that intrapersonal distances (distances between different photos of the same person) are small and interpersonal distances (distances between photos of different people) are large. In the linear case, this is equivalent to finding a symmetric positive definite matrix &lt;span class="math"&gt;\({M}\)&lt;/span&gt; such that, given &lt;span class="math"&gt;\({x_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\({x_1}\)&lt;/span&gt;  :&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
d({x_1}, {x_2}) = \sqrt{{(x_1-x_2)}^T{M}{(x_1-x_2)}}
\end{equation}&lt;/div&gt;
&lt;p&gt;satisfies the properties above. This metric is called a Mahalanobis distance, and is has seen wide use in statistics in general and face verification and recognition in particular, specially in combination with Principal Component Analysis as in &lt;a href='#mahalanobis' id='ref-mahalanobis-1'&gt;(Fan et al., 2013)&lt;/a&gt;. The characterization of M allows is to write it as a product of another matrix and its transpose, and so &lt;span class="math"&gt;\((1)\)&lt;/span&gt; is equivalent to:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
d({x_1}, {x_2}) = ||{W}{x_1}-{W}{x_2}||
\end{equation}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\({M}={W}^T{W}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;By the manifold hypothesis, however, face space would have a manifold structure on pixel space, which cannot be adequately captured by linear transformations &lt;a href='#ddml' id='ref-ddml-1'&gt;(Hu et al., 2014)&lt;/a&gt;. One possible solution is to use a neural network to learn a function whose role is analogous to that of &lt;span class="math"&gt;\({W}\)&lt;/span&gt; in the above example, but not restricted to linear maps. This is the option we explore in this section, and to this end we use what's called a Siamese CNNs architecture.&lt;/p&gt;
&lt;p&gt;The idea of the Siamese CNNs architecure &lt;a href='#siamese-cnns' id='ref-siamese-cnns-1'&gt;(Chopra et al., 2005)&lt;/a&gt; is to train two identical CNNs that share parameters, and whose outputs are fed to an energy function that will measure how "dissimilar" they are, upon which we'll then compute our loss function. Gradient descent on this loss propagates to the two CNNs in the same way, preserving the symmetry of the problem. &lt;/p&gt;
&lt;figure&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/siamese-cnns.jpg" align='middle'/&gt;
  &lt;figcaption&gt;&lt;sup&gt; Scheme of the Siamese CNNs architecture (source: &lt;a href='#siamese-cnns' id='ref-siamese-cnns-2'&gt;(Chopra et al., 2005)&lt;/a&gt;). &lt;/sup&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In our implementation, each CNN is comprised of three convolutions, all of kernel size 6x6, and computing respectively 5, 14 and 60 features, followed by a fully-connected layer that computes 40 features. Convolutions 1 and 2 are also followed by 2x2 max-pooling layers.&lt;/p&gt;
&lt;h1&gt;DeepID&lt;/h1&gt;
&lt;p&gt;Another way to tackle verification is to think of it as a subproblem of face identification, that is, the classification problem that involves assigning to each person a label: their identity. In the case of face verification, we're just trying to know if this assignment is the same for two given points in our dataset.&lt;/p&gt;
&lt;p&gt;The jump from verification to identification can certainly be impractical: in our earlier example of biometrics, for instance, in order to prevent the entrance of undesired people, the owner of the system would ideally have to train his algorithm to recognize all seven billion people on earth. Far from this naive approach, however, lies an interesting connection that makes the exploration of this harder problem worthwhile: both problems are based on the recognition of facial features, so training a neural network to perform the hard problem of identification can in principle give very good descriptors for verification. That is the core idea behind DeepID &lt;a href='#deepid' id='ref-deepid-1'&gt;(Sun et al., 2014)&lt;/a&gt;, a state-of-the-art algorithm for face verification.&lt;/p&gt;
&lt;p&gt;DeepID implements a CNN with four convolutional layers, of kernel sizes 4x4, 3x3, 2x2 and 2x2 and computing 20, 40, 60 and 80 features respectively. The first three layers are followed by 2x2 max-pooling, and both convolution 3 and 4 output to a fully-connected layer (named after the algorithm itself) that computes 160 features and will be used for the verification task later. Finally, for the identification task, the final layer is a softmax for classification between all the identities in the dataset.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/sparse-face-verification/master/images/deepid.jpg" align='middle'/&gt;
  &lt;figcaption&gt;&lt;sup&gt; Visualization of the DeepID architecture (source: &lt;a href='#deepid' id='ref-deepid-2'&gt;(Sun et al., 2014)&lt;/a&gt;). &lt;/sup&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;After training on the identification task, we can remove the softmax layer and use the fully-connected DeepID layer as a descriptor for an algorithm that will perform verification on a 160-dimensional space. In Sun's paper, the method found to have the best results was the joint-bayesian model.&lt;/p&gt;
&lt;p&gt;Joint-bayesian models &lt;a href='#joint-bayesian' id='ref-joint-bayesian-1'&gt;(Chen et al., 2012)&lt;/a&gt; the class centers &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; as well as the intra-class variations &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; both follow a centered gaussian distributions, whose covariance matrices &lt;span class="math"&gt;\(S_\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(S_\epsilon\)&lt;/span&gt; are the objects we're trying to infer from the data.&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
x = \mu + \epsilon, \; \; \mu\sim\mathcal{N}\big(0, S_\mu), \; \; \epsilon\sim\mathcal{N}\big(0, S_\epsilon)
\end{equation}&lt;/div&gt;
&lt;p&gt;Given two observations &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt;, if we call &lt;span class="math"&gt;\(H_I\)&lt;/span&gt; the hypothesis that they represent the face of the same person and &lt;span class="math"&gt;\(H_E\)&lt;/span&gt; the hypothesis that they come from different people, we can easily see that under &lt;span class="math"&gt;\(H_I\)&lt;/span&gt;, &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; share the same class center and have independent intra-class variation, while under &lt;span class="math"&gt;\(H_E\)&lt;/span&gt;, both their class center and intra-class variation are independent. This leads us to the conclusion that the covariance between &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; under &lt;span class="math"&gt;\(H_I\)&lt;/span&gt; and &lt;span class="math"&gt;\(H_E\)&lt;/span&gt; are respectively:&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\Sigma_I = \begin{bmatrix} S_\mu+S_\epsilon &amp;amp; S_\mu\\ S_\mu &amp;amp; S_\mu+S_\epsilon\end{bmatrix}, \; \; \Sigma_E = \begin{bmatrix} S_\mu+S_\epsilon &amp;amp; 0\\ 0 &amp;amp; S_\mu+S_\epsilon\end{bmatrix}
\end{equation}&lt;/div&gt;
&lt;p&gt;The covariance matrices are learned jointly through Expectation-Maximization (EM), an algorithm for estimating the maximum likelihood parameter in a latent variable model through iteration of an E-step, in which we compute the distribution of the latent variables using our previous guess for the parameter, and an M-step, in which we update the parameter so as to maximize the joint distribution likelihood (for more on EM, some great notes by Andrew NG can be found &lt;a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf"&gt;here&lt;/a&gt;). The log likelihood here is given by &lt;span class="math"&gt;\(r(x_1, x_2) = \log{\frac{P(x_1, x_2 | H_I)}{P(x_1, x_2 | H_E)}}\)&lt;/span&gt;, and using the equation above  we arrive at a closed form for it whose solution can be computed efficiently. &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='joint-bayesian'&gt;D.&amp;nbsp;Chen, X.&amp;nbsp;Cao, L.&amp;nbsp;Wang, F.&amp;nbsp;Wen, and Sun. J.
Bayesian face revisited: a joint formulation.
In &lt;em&gt;Proc. ECCV&lt;/em&gt;. 2012. &lt;a class="cite-backref" href="#ref-joint-bayesian-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='siamese-cnns'&gt;S.&amp;nbsp;Chopra, R.&amp;nbsp;Hadsell, and Y.&amp;nbsp;LeCun.
Learning a similarity metric discriminatively, with application to face verification.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2005. &lt;a class="cite-backref" href="#ref-siamese-cnns-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-siamese-cnns-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-siamese-cnns-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='hog'&gt;N.&amp;nbsp;Dallal and B.&amp;nbsp;Trigs.
Histograms of oriented gradients for human detection.
In &lt;em&gt;Proc. ICCV&lt;/em&gt;. 2005. &lt;a class="cite-backref" href="#ref-hog-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='mahalanobis'&gt;Z.&amp;nbsp;Fan, M.&amp;nbsp;Ni, M.&amp;nbsp;Sheng, and Z.&amp;nbsp;Wu.
Principal component analysis integrating mahalanobis distance for face recognition.
In &lt;em&gt;Proc. RVSP&lt;/em&gt;. 2013. &lt;a class="cite-backref" href="#ref-mahalanobis-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='ddml'&gt;J.&amp;nbsp;Hu, J.&amp;nbsp;Lu, and Y.-P. Tan.
Discriminative deep metric learning for face verification in the wild.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2014. &lt;a class="cite-backref" href="#ref-ddml-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='title-image'&gt;M.&amp;nbsp;Köstinger, M.&amp;nbsp;Hirzer, P.&amp;nbsp;Wohlhart, and H.&amp;nbsp;Bischof.
Large scale metric learning from equivalence constraints.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2012. &lt;a class="cite-backref" href="#ref-title-image-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='exemplar-svm'&gt;T.&amp;nbsp;Malisiewicz, A.&amp;nbsp;Gupta, and A.&amp;nbsp;A. Efros.
Ensemble of exemplar-svms for object detection and beyond.
In &lt;em&gt;Proc. ICCV&lt;/em&gt;. 2011. &lt;a class="cite-backref" href="#ref-exemplar-svm-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-exemplar-svm-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-exemplar-svm-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='deepid'&gt;Y.&amp;nbsp;Sun, W.&amp;nbsp;Xiaogang, and T.&amp;nbsp;Xiaoou.
Deep learning face representation from predicting 10,000 classes.
In &lt;em&gt;Proc. CVPR&lt;/em&gt;. 2014. &lt;a class="cite-backref" href="#ref-deepid-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-deepid-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-deepid-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
</summary><category term="face verification"></category><category term="convolutional neural networks"></category><category term="exemplar svms"></category></entry><entry><title>Brownian Motion</title><link href="/brownian-motion.html" rel="alternate"></link><updated>2016-04-02T14:54:00+02:00</updated><author><name>João Loula</name></author><id>tag:,2016-04-02:brownian-motion.html</id><summary type="html">&lt;p&gt;In this post, we'll explore different characteristics of the Brownian motion, going from the discrete to the continuous case, and finally expressing the particle's behavior in large times. This model has been the subject of various applications in different areas, like biochemistry, medical imaging and financial markets, and the domain has shown itself to be a fertile subject in mathematics, with important contributions from the likes of Paul Levy, Norbert Wiener and Albert Einstein.&lt;/p&gt;
&lt;p&gt;Code for this post can be found &lt;a href="https://github.com/Joaoloula/joaoloula.github.io-src/tree/master/content/posts/brownian-motion/code"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Discovering the Brownian movement&lt;/h2&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(X_i\)&lt;/span&gt; be a sequence of Bernoulli random variables that take the values &lt;span class="math"&gt;\(-1\)&lt;/span&gt; and &lt;span class="math"&gt;\(1\)&lt;/span&gt; with equal probability. We can then define the random variable &lt;span class="math"&gt;\(B_n\)&lt;/span&gt; as: &lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/brown_definition.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;We can show, using the central limit theorem, that &lt;span class="math"&gt;\(B_n\)&lt;/span&gt; converges in law to a centered normal distribution of variance &lt;span class="math"&gt;\(t\)&lt;/span&gt; when &lt;span class="math"&gt;\(n\)&lt;/span&gt; goes to infinity. Taking this limit in a vector &lt;span class="math"&gt;\((B_n(t))\)&lt;/span&gt; whose entries are &lt;span class="math"&gt;\(B_n\)&lt;/span&gt; calculated at increasing times, we obtain the stochastic process &lt;span class="math"&gt;\((B(t))\)&lt;/span&gt;, that we call the Brownian movement. Its first remarkable property is that, by construction, given &lt;span class="math"&gt;\(t\)&lt;/span&gt; bigger than &lt;span class="math"&gt;\(s\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/brown_property.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;It can also be proved that it is continuous but nowhere differentiable. Let's get an idea for what it looks like. In one dimension, for &lt;span class="math"&gt;\(n=100\)&lt;/span&gt; and &lt;span class="math"&gt;\(t=100\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S4-(n=100, t=100).png"/&gt;
&lt;/p&gt;

&lt;p&gt;In two and three dimensions, for &lt;span class="math"&gt;\(n=100\)&lt;/span&gt; and &lt;span class="math"&gt;\(t=1000\)&lt;/span&gt; (the time axis is now ommited):&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S5-2d-(n=100, t=1000).png"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/cover.png"/&gt;
&lt;/p&gt;

&lt;p&gt;Quite the looker, isn't it? We'll now try a more quantitative analysis to get a feel for what's going on.&lt;/p&gt;
&lt;h2&gt;Position and speed distribution in discrete time&lt;/h2&gt;
&lt;p&gt;Robert Brown, the botanist after whom the process is named, first discovered it when observing the movement of pollen particules suspended in water: the random collisions of water mollecules with the particules created a very irregular movement that he was incapable of explaining. The phenomenon was later explained in detail by Einstein in a 1905 paper [1]. The model can be explained as follows: the acceleration of the particule is proportional to the force exercised by the random collisions; there's also a friction force proportional to the particle's speed (we'll call this proportionnality factor lambda). We'll ignore weight as well as buoyancy: we can get away with this because the pollen particle is really small.&lt;/p&gt;
&lt;p&gt;We'll introduce a time interval &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, during which the particle suffers &lt;span class="math"&gt;\(N \alpha\)&lt;/span&gt; collisions (which we suppose to be a very large number). The collisions (which we'll identify by &lt;span class="math"&gt;\(\epsilon_i\)&lt;/span&gt;) are supposed to be independent and identically distributed of random intensity centered at &lt;span class="math"&gt;\(0\)&lt;/span&gt; and square integrable (of variance &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;). We obtain the following expression then for the evolution of the particle's velocity:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/pollen_model.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;Where &lt;span class="math"&gt;\(Y\)&lt;/span&gt; is a random variable of centered gaussian distribution with variance one (which we can infer from applying the central limit theorem to the first expression). To make the notation less cluttered, we'll set &lt;span class="math"&gt;\(a = -\lambda\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(b = \sqrt{N\sigma^2\alpha}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The interest of this formulation is that we can now proceed to quantitaive analysis of the position &lt;span class="math"&gt;\(X_n\)&lt;/span&gt; and velocity &lt;span class="math"&gt;\(V_n\)&lt;/span&gt; of a particle subject to brownian movement (&lt;span class="math"&gt;\(X_n\)&lt;/span&gt; is just a "discrete integration" of the velocity, that is, a &lt;span class="math"&gt;\(\sum\limits_{i} V_i\alpha\)&lt;/span&gt;). &lt;/p&gt;
&lt;p&gt;We can now show that &lt;span class="math"&gt;\(X_n\)&lt;/span&gt; and &lt;span class="math"&gt;\(V_n\)&lt;/span&gt; follow gaussian distributions: even better, by exploiting the independence of  &lt;span class="math"&gt;\((Y_i)\)&lt;/span&gt;, we can prove that any linear combination of &lt;span class="math"&gt;\(X_n\)&lt;/span&gt; and &lt;span class="math"&gt;\(V_n\)&lt;/span&gt; also follows a gaussian distribution, which allows us to conclude that &lt;span class="math"&gt;\((X_n, Y_n)\)&lt;/span&gt; is a gaussian vector. If we note the initial position and velocity by &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(v\)&lt;/span&gt; respectively, its mean is given by:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/xn_vn_mean.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;Now that we're all set up, let's see what happens when we play around with the parameters. This is a simulation for &lt;span class="math"&gt;\(a = 0.5\)&lt;/span&gt; and &lt;span class="math"&gt;\(b = 1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S8-(a=0.5, b=1).png"/&gt;
&lt;/p&gt;

&lt;p&gt;This one, for &lt;span class="math"&gt;\(a = 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(b = 0.1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S8-(a=1, b=0.1).png"/&gt;
&lt;/p&gt;

&lt;p&gt;And finally, for &lt;span class="math"&gt;\(a = 0.1\)&lt;/span&gt; and &lt;span class="math"&gt;\(b = 10\)&lt;/span&gt;:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S8-(a=0.1, b=10).png"/&gt;
&lt;/p&gt;

&lt;p&gt;What we see is that the term &lt;span class="math"&gt;\(a\)&lt;/span&gt; controls the influence of the previous velocity over the next one (the "memory" of the process) and &lt;span class="math"&gt;\(b\)&lt;/span&gt;, it's degree of randomness, which goes well with our characterization of these parameters earlier (multiplying previous velocity and a random gaussian variable in the update rule, respectively). We see, in the first simulation, a random process with an upwards drift, explained by the memory of the initial velocity. In the second one, we have an almost perfect exponential, since the random character of the curve is almost suppressed by the small &lt;span class="math"&gt;\(b\)&lt;/span&gt; value. In the final simulation, however, we have almost no memory and the process is mostly chaotic. This last one is then more likely the one that resembles the most the results that Brown found, and we can use the discrete integration process described earlier to simulate the movement of a pollen particule in the surface of a water tank based on these parameters:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S8-3-pollen2d.png"/&gt;
&lt;/p&gt;

&lt;h2&gt;Continuous time&lt;/h2&gt;
&lt;p&gt;So we've gotten some pretty interesting results so far for our model of Brownian movement, but you might be thinking our formulation is not yet satisfactory for a large range of problems: most systems do not behave in an organized discrete manner, weighing inputs like they were votes and then deciding which action to take at each discrete time step, but are rather complicated phenomena that are at all times being influenced by different factors in a &lt;em&gt;continuous&lt;/em&gt; way -- or at least, like stock markets, chains of events that happen so quickly with respect to our measuring instruments and reaction times that they might as well be continuous. We can then ask ourselves if there's a rigorous mathematical way in which we can expand Brownian motion to continuous time. That's the problem we'll address in this section.&lt;/p&gt;
&lt;p&gt;We'll take &lt;span class="math"&gt;\(K\)&lt;/span&gt; to be the inverse of the timestep &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;. We know then, by the previous section's results, that for a fixed &lt;span class="math"&gt;\(K\)&lt;/span&gt;, &lt;span class="math"&gt;\((X_t^K, Y_t^K)\)&lt;/span&gt; a gaussian vector. Their characteristic function &lt;span class="math"&gt;\(\phi_{X_t^K, Y_t^K}\)&lt;/span&gt; is given by:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/phi.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;Where &lt;span class="math"&gt;\(C\)&lt;/span&gt; is the covariance matrix of the position and velocity random variables. We can calculate the limits when &lt;span class="math"&gt;\(K\)&lt;/span&gt; goes to infinity of the two expected values from the formulas we found for the expected values in the previous section:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/vn_limit.jpg"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/xn_limit.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;The limit of the covariance matrix is a little bit more complicated, and so we'll omit it here: it suffices to know that it converges. We then have convergence of the characteristic function to another function &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; that's also the characteristic function of a gaussian. By Lebesgue's dominated convergence theorem, we know that &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; is continuous at zero, and so we can conclude by Levy's theorem that &lt;span class="math"&gt;\((X_t^K, Y_t^K)\)&lt;/span&gt; converges in distribution to a gaussian vector.&lt;/p&gt;
&lt;p&gt;In the continuous limit, velocity variance is well-behaved with respect to time, the variance of the position random variable, however, diverges. To verify this point, we'll plot histograms of &lt;span class="math"&gt;\(X\)&lt;/span&gt; values in the continuous approximation for the times 100, 1000 and 10000, while paying attention to the axes:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S10-1-(t=100).png"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S10-(t=1000).png"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S10-2-(t=10000).png"/&gt;
&lt;/p&gt;

&lt;p&gt;We can se that the positions spread wider and wider as we increase the timescale. &lt;/p&gt;
&lt;h2&gt;A kinetic model&lt;/h2&gt;
&lt;p&gt;This sort of behavior for the position variable, while corresponding to the idea of a free evolution of the particle, is not always desirable. More precisely, in many physical systems, the particle's a priori unbounded movement is counteracted by a force that attracts it to a center (a charged particle, a massive body). This is the sort of model we'll investigate in this section. With this goal in mind, we introduce new velocity and position random variables defined by:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/vn_final.jpg"/&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/equations/xn_final.jpg"/&gt;
&lt;/p&gt;

&lt;p&gt;Where &lt;span class="math"&gt;\(D\)&lt;/span&gt; is called the diffusion coefficient and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; is a strictly positive value.&lt;/p&gt;
&lt;p&gt;The problem with this formulation is that now that the variables are coupled it is quite tricky to solve the system. A more interesting approach would be to do a spectral analysis of the recurrence matrix: for large values of &lt;span class="math"&gt;\(K\)&lt;/span&gt;, the anti-diagonal is dominated by the term &lt;span class="math"&gt;\(-\beta\)&lt;/span&gt;, that plays a regulatory role in the system, and we expect to see imaginary eigenvalues that will define an envelope curve as well as a strong oscillatory behavior. We can run a simulation of the system with large initial conditions in &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(V\)&lt;/span&gt; to verify its behavior:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S13 - (position estabilize jolie).png"/&gt;
&lt;/p&gt;

&lt;p&gt;Besides confirming our hypothesis as we can see from the exponential envelope and sinusoidal oscillations, it is worth noting that the initial conditions become meaningless over time: the system is always brought close to the center of oscillation. What is perhaps most interesting, though, is that the system does not get arbitrarily close to the center, that is, the motion does not converge, as we can see in this simulation with zero initial condition:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/brownian-motion/simulations/S13-(position t=100, K=25).png"/&gt;
&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] http://users.physik.fu-berlin.de/~kleinert/files/eins_brownian.pdf&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="brownian motion"></category><category term="stochastic process"></category><category term="physics"></category></entry><entry><title>Functional brain atlas with Nilearn</title><link href="/functional-atlas.html" rel="alternate"></link><updated>2016-04-02T14:54:00+02:00</updated><author><name>João Loula</name></author><id>tag:,2016-04-02:functional-atlas.html</id><summary type="html">&lt;p&gt;This post is based on the Nilearn tutorial given by myself and Alex Abraham at the 2016 Brainhack Vienna: in it, we'll give a brief introduction to Nilearn and its functionalities, and we'll present a usecase of extracting a functional brain atlas from the ABIDE resting state dataset.&lt;/p&gt;
&lt;h2&gt;Nilearn&lt;/h2&gt;
&lt;p&gt;Nilearn is a python module for statistical and machine learning analysis on brain data: it leverages python's simplicity and versatility into an easy-to-use integrated pipeline. Having analysis run on single, simple scripts allows for better reproducibility than, say, clicking on things in a GUI.&lt;/p&gt;
&lt;p&gt;This is how a typical Nilearn analysis goes:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/functional-atlas/nilearn_candy.png" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;source: &lt;a href='#nilearn-poster' id='ref-nilearn-poster-1'&gt;(Abraham et al., 2016)&lt;/a&gt;&lt;/sup&gt;
&lt;/figure&gt;

&lt;p&gt;One of the main objects in the module is the Masker: it allows for easy conversion from a 4D brain scan time-series to a numpy array that's ready to be treated by scikit-learn algorithms and vice-versa. Accompanying it are a wide range of image processing functions, allowing for flexible data manipulation.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/functional-atlas/masking.jpg" alt='missing' align='middle' /&gt;
    &lt;figcaption&gt; &lt;sup&gt;source: &lt;a href='#nilearn-poster' id='ref-nilearn-poster-2'&gt;(Abraham et al., 2016)&lt;/a&gt;&lt;/sup&gt;
&lt;/figure&gt;

&lt;p&gt;Next, we'll take a look at a use case to see how the module works in action, on the ABIDE autism resting-state data &lt;a href='#abide' id='ref-abide-1'&gt;(DiMartino et al., 2014)&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Brain atlas&lt;/h2&gt;
&lt;p&gt;Before analyzing functional connectivity, we need to reduce the dimensionality of the problem. To do that, we estimate an atlas directly on our data. We'll start by importing some classic libraries:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# This line allows plotting directly in the notebook&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;

&lt;span class="c1"&gt;# Python scientific package&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Loading the data&lt;/h3&gt;
&lt;p&gt;Nilearn provides a bunch of automatic downloaders to ease reproducibility of the analysis. With nilearn, an analysis is run in a single script and can be shared easily. The nilearn fetchers can be found in the module nilearn.datasets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nilearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fetch_abide_pcp&lt;/span&gt;


&lt;span class="c1"&gt;# We specify the site and number of subjects we want to download&lt;/span&gt;
&lt;span class="n"&gt;abide&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_abide_pcp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;derivatives&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;func_preproc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
                        &lt;span class="n"&gt;SITE_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;NYU&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
                        &lt;span class="n"&gt;n_subjects&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# We look at the available data in this dataset&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can print a description of the dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Retrieving the functional dataset is also straightforward:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# To get the functional dataset, we have to retrieve the variable &amp;#39;func_preproc&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;func&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;abide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;func_preproc&lt;/span&gt;

&lt;span class="c1"&gt;# We can also look at where the data is loaded&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Computing a brain atlas&lt;/h3&gt;
&lt;p&gt;Several reference atlases are available in nilearn. We also provide functions to compute a brain atlas directly from the data. In this example, we'll do this using a group ICA implementation called Canonical ICA.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nilearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;decomposition&lt;/span&gt;

&lt;span class="c1"&gt;# CanICA is nilearn&amp;#39;s approach of group ICA. It directly embeds a masker.&lt;/span&gt;
&lt;span class="n"&gt;canica&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decomposition&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CanICA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mask_strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;background&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;canica&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Retrieve the components&lt;/span&gt;
&lt;span class="n"&gt;components&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;canica&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;

&lt;span class="c1"&gt;# Use CanICA&amp;#39;s masker to project the components back into 3D space&lt;/span&gt;
&lt;span class="n"&gt;components_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;canica&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;masker_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inverse_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;components&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# We visualize the generated atlas&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nilearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;

&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_stat_map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;components_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;DMN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/functional-atlas/DMN.png"/&gt;
&lt;/p&gt;

&lt;h3&gt;Extracting subject specific timeseries signals from brain parcellations&lt;/h3&gt;
&lt;p&gt;Computing mask from the data, filtering, extracting data from the in-mask voxels can be processed easily by using nilearn classes such as NiftiMasker, NiftiMapsMasker, NiftiLabelsMasker which can be imported from nilearn.input_data module.
The advantage of using such tools from this module is that we can restrict our analysis to mask specific voxels timeseries data. For instance, class NiftiMasker can be used to compute mask over the data and apply preprocessing steps such as filtering, smoothing, standardizing and detrending on voxels timeseries signals. This type of processing is very much necessary, particularly during resting state fMRI data analysis. Additional to NiftiMasker, classes NiftiMapsMasker and NiftiLabelsMasker, can be used to extract subject specific timeseries signals on each subject data provided with the atlas maps (3D or 4D) comprising of specific brain regions. NiftiMapsMasker operated on 4D atlas maps, can be used to extract signals from each 4th dimensional map using least squares regression. Whereas, NiftiLabelsMasker operated on 3D maps denoted as labels image, can be used to extract averaged timeseries from group of voxels that correponds to each label in the image.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Import and initialize `NiftiMapsMasker` object and call `fit_transform` to&lt;/span&gt;
&lt;span class="c1"&gt;# extract timeseries signals from computed atlas.&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nilearn.input_data&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;NiftiMapsMasker&lt;/span&gt;

&lt;span class="c1"&gt;# The parameters used are maps_img as parcellations, resampling to maps image,&lt;/span&gt;
&lt;span class="c1"&gt;# smoothing of 6mm, detrending, standardizing and filtering (TR in sec). These later&lt;/span&gt;
&lt;span class="c1"&gt;# parameters are applied automatically when extracting timeseries data.&lt;/span&gt;
&lt;span class="n"&gt;masker&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;NiftiMapsMasker&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;components_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;smoothing_fwhm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                         &lt;span class="n"&gt;standardize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;detrend&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;t_r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;low_pass&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                         &lt;span class="n"&gt;high_pass&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Extracting time series for each subject&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# We loop over the subjects to extract the time series&lt;/span&gt;
&lt;span class="n"&gt;subjects_timeseries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;subject_func&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;subjects_timeseries&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;masker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subject_func&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Visualizing extracted timeseries signals. We import matplotlib.pyplot&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;


&lt;span class="c1"&gt;# We loop over the subjects to extract the time series&lt;/span&gt;
&lt;span class="c1"&gt;# We show them for a single subject&lt;/span&gt;
&lt;span class="n"&gt;timeseries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subjects_timeseries&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeseries&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# (number of scans/time points, number of brain regions/parcellations)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeseries&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Timeseries for single subject shown for 20 brain regions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Number of regions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Normalized signal&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/functional-atlas/time-series.png"/&gt;
&lt;/p&gt;

&lt;h3&gt;Extracting regions from computed atlas&lt;/h3&gt;
&lt;p&gt;ICA requires post-preprocessing. Here we use the RegionExtractor that thresholds the maps and extract brain regions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nilearn.regions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RegionExtractor&lt;/span&gt;


&lt;span class="n"&gt;extractor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RegionExtractor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;components_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;thresholding_strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
                            &lt;span class="s1"&gt;&amp;#39;ratio_n_voxels&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;extractor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;local_regions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;standardize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                            &lt;span class="n"&gt;min_region_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1350&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Just call fit() to process for regions extraction&lt;/span&gt;
&lt;span class="n"&gt;extractor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Extracted regions are stored in regions_img_&lt;/span&gt;
&lt;span class="n"&gt;regions_extracted_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extractor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regions_img_&lt;/span&gt;

&lt;span class="c1"&gt;# Total number of regions extracted&lt;/span&gt;
&lt;span class="n"&gt;n_regions_extracted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;regions_extracted_img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Visualization of region extraction results&lt;/span&gt;
&lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; regions are extracted from &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; components.&amp;#39;&lt;/span&gt;
         &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_regions_extracted&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_prob_atlas&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;regions_extracted_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                         &lt;span class="n"&gt;view_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;filled_contours&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.008&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Connectomes Estimation&lt;/h3&gt;
&lt;p&gt;Connectivity is typically estimated using correlation between time series. Recent studies has shown that partial correlation could give better results. Different estimators can also be used to apply some regularization on the matrix coefficients. Nilearn's ConnectivityMeasure object (in the nilearn.connectome module) provides three types of connectivity matrix: correlation, partial_correlation, and tangent (a method developped in our laboratory). ConnectivityMeasure can also use any covariance estimator shipped by scikit-learn (ShrunkCovariance, GraphLasso). In a first time, we estimate the connectivity using default parameters. We check that we have one matrix per subject.&lt;/p&gt;
&lt;p&gt;from nilearn.connectome import ConnectivityMeasure&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;conn_est&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConnectivityMeasure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;partial correlation&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conn_matrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conn_est&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rois_cc200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Plotting connectivity matrix&lt;/h3&gt;
&lt;p&gt;We visualize the connectivity matrix of the first subject. This code is directly taken from a nilearn example.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conn_matrices&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;vmax&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vmin&lt;/span&gt;&lt;span class="o"&gt;=-.&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;RdBu_r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colorbar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Connectivity matrix of subject 0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p align="center"&gt;
  &lt;img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/functional-atlas/connectivity-matrix.png"/&gt;
&lt;/p&gt;

&lt;h3&gt;Extracting useful coefficients&lt;/h3&gt;
&lt;p&gt;Connecitivity matrices are symmetric. As such, half of the coefficients are redundant. They can even impact the results of some predictors. In order to "extract" these coefficients, we want to use a mask. numpy.tril function can help us with this task. However, using masking is hazardous without a good knowledge of numpy. Fortunately, nilearn provides a function to do this automatically and efficiently: nilearn.connectome.sym_to_vec.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nilearn.connectome&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sym_to_vec&lt;/span&gt;


&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sym_to_vec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conn_matrices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Setting up cross-validation&lt;/h3&gt;
&lt;p&gt;Getting reliable prediction results require to predict on unseen data. Cross-validation consists in leaving out a part of the dataset (testing set) to validate the model learnt on the remaining of the dataset (training set). Scikit-learn has all the utils necessary to do automatic cross-validation. In the case of ABIDE, we have a very heterogenous dataset and we want the sets to be balanced in term of acquisition sites and condition. We use a stratified cross-validation method for that.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StratifiedShuffleSplit&lt;/span&gt;


&lt;span class="n"&gt;ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;site_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;abide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;phenotypic&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;SITE_ID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;DX_GROUP&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]:&lt;/span&gt;
    &lt;span class="n"&gt;ids&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;site_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dx&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StratifiedShuffleSplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Prediction using Support Vector Classifier&lt;/h3&gt;
&lt;p&gt;Now that we have shown how to estimate a connectome and extract the interesting coefficients, we will see how to use them to diagnose ASD vs healthy individuals. For that purpose, we use a Support Vector Machine. This is one of the most simple classifiers. We use the default parameters in a first time and look at classification scores.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearSVC&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;


&lt;span class="c1"&gt;# DX_GROUP are the labels of the ABIDE dataset. 1=ASD, 2=Healthy&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;abide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;phenotypic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;DX_GROUP&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;predictor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearSVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Exploring other methods and parameters&lt;/h3&gt;
&lt;p&gt;So far, we built a basic prediction procedure without tuning the parameters. Now we use for loops to explore several options. Note that the imbrication of the steps allow us to re-use connectivity matrix computed in the first loop for the different predictors. The same result can be achieved using nilearn's caching capacities.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RidgeClassifier&lt;/span&gt;


&lt;span class="n"&gt;measures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;correlation&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;partial correlation&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tangent&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;predictors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;svc_l2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LinearSVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;svc_l1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LinearSVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;penalty&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;l1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dual&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ridge_classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RidgeClassifier&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;measure&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;measures&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;conn_est&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConnectivityMeasure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;measure&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;conn_matrices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conn_est&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rois_cc200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sym_to_vec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conn_matrices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictor&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;predictors&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;measure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This should show the Ridge Classifier and the SVM classifier with L1 penalty as the highest scoring options.&lt;/p&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='nilearn-poster'&gt;Alexandre Abraham, Loïc Estève, Elvis Dohmatob, Danilo Bzdok, Kamalakar Reddy, Arthur Mensch, Philippe Gervais, Virgile Fritsch, Salma Bougacha, Ben Cipollini, Mehdi Rahim, Martin Perez-Guevara, Krzysztof Gorgolewski, Óscar Nájera, Michael Eickenberg, Alexandre Abadie, Yannick Schwartz, Andrés Andrés&amp;nbsp;Hoyos Idrobo, Konstantin Shmelkov, Fabian Pedregosa, Andreas Mueller, Jean Kossaifi, Jaques Grobler, Alexandre Gramfort, Michael Hanke, Bertrand Thirion, and Gael Varoquaux.
Nilearn: machine learning for neuro-imaging in python.
&lt;em&gt;OHBM&lt;/em&gt;, 2016. &lt;a class="cite-backref" href="#ref-nilearn-poster-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-nilearn-poster-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-nilearn-poster-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='abide'&gt;A.&amp;nbsp;Di&amp;nbsp;Martino, C.&amp;nbsp;G. Yan, Q.&amp;nbsp;Li, E.&amp;nbsp;Denio, F.&amp;nbsp;X. Castellanos, K.&amp;nbsp;Alaerts, J.&amp;nbsp;S. Anderson, M.&amp;nbsp;Assaf, S.&amp;nbsp;Y. Bookheimer, M.&amp;nbsp;Dapretto, B.&amp;nbsp;Deen, S.&amp;nbsp;Delmonte, I.&amp;nbsp;Dinstein, B.&amp;nbsp;Ertl-Wagner, D.&amp;nbsp;A. Fair, L.&amp;nbsp;Gallagher, D.&amp;nbsp;P. Kennedy, C.&amp;nbsp;L. Keown, C.&amp;nbsp;Keysers, J.&amp;nbsp;E. Lainhart, C.&amp;nbsp;Lord, B.&amp;nbsp;Luna, V.&amp;nbsp;Menon, N.&amp;nbsp;J. Minshew, C.&amp;nbsp;S. Monk, S.&amp;nbsp;Mueller, R.&amp;nbsp;A. Müller, M.&amp;nbsp;B. Nebel, J.&amp;nbsp;T. Nigg, K.&amp;nbsp;O'Hearn, K.&amp;nbsp;A. Pelphrey, S.&amp;nbsp;J. Peltier, J.&amp;nbsp;D. Rudie, S.&amp;nbsp;Sunaert, M.&amp;nbsp;Thioux, J.&amp;nbsp;M. Tyszka, L.&amp;nbsp;Q. Uddin, J.&amp;nbsp;S. Verhoeven, N.&amp;nbsp;Wenderoth, J.&amp;nbsp;L. Wiggins, S.&amp;nbsp;H. Mostofsky, and M.&amp;nbsp;P. Milham.
The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism.
&lt;em&gt;Mol Psychiatry&lt;/em&gt;, 2014. &lt;a class="cite-backref" href="#ref-abide-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</summary><category term="functional atlas"></category><category term="fmri"></category><category term="resting state"></category></entry><entry><title>Brain surface plotting with Nilearn</title><link href="/surface-plotting.html" rel="alternate"></link><updated>2016-04-02T14:54:00+02:00</updated><author><name>João Loula</name></author><id>tag:,2016-04-02:surface-plotting.html</id><summary type="html">&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nilearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;plotting&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nilearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;stats&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nibabel&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nb&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Retrieve the data&lt;/span&gt;
&lt;span class="n"&gt;nki_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fetch_surf_nki_enhanced&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_subjects&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# NKI resting state data set of one subject left hemisphere in fsaverage5 space&lt;/span&gt;
&lt;span class="n"&gt;resting_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nki_dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;func_left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="sb"&gt;``&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;python&lt;/span&gt;

&lt;span class="sb"&gt;``&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;python&lt;/span&gt;
&lt;span class="c1"&gt;# Destrieux parcellation left hemisphere in fsaverage5 space&lt;/span&gt;
&lt;span class="n"&gt;destrieux_atlas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fetch_atlas_surf_destrieux&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;parcellation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;freesurfer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_annot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;destrieux_atlas&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;annot_left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Retrieve fsaverage data&lt;/span&gt;
&lt;span class="n"&gt;fsaverage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fetch_surf_fsaverage5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Fsaverage5 left hemisphere surface mesh files&lt;/span&gt;
&lt;span class="n"&gt;fsaverage5_pial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fsaverage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pial_left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;fsaverage5_inflated&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fsaverage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;infl_left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sulcal_depth_map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fsaverage&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sulc_left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Load resting state time series and parcellation&lt;/span&gt;
&lt;span class="n"&gt;timeseries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;surf_plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;check_surf_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resting_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Extract seed region: dorsal posterior cingulate gyrus&lt;/span&gt;
&lt;span class="n"&gt;region&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;G_cingul-Post-dorsal&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;parcellation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;parcellation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;region&lt;/span&gt;&lt;span class="p"&gt;))[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Extract time series from seed region&lt;/span&gt;
&lt;span class="n"&gt;seed_timeseries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeseries&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Calculate Pearson product-moment correlation coefficient between seed&lt;/span&gt;
&lt;span class="c1"&gt;# time series and timeseries of all cortical nodes of the hemisphere&lt;/span&gt;
&lt;span class="n"&gt;stat_map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeseries&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeseries&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="n"&gt;stat_map&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pearsonr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed_timeseries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timeseries&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Re-mask previously masked nodes (medial wall)&lt;/span&gt;
&lt;span class="n"&gt;stat_map&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeseries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Display ROI on surface&lt;/span&gt;
&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_surf_roi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fsaverage5_pial&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roi_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hemi&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       &lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;medial&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bg_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sulcal_depth_map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bg_on_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Display unthresholded stat map in lateral and medial view&lt;/span&gt;
&lt;span class="c1"&gt;# dimmed background&lt;/span&gt;
&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_surf_stat_map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fsaverage5_pial&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stat_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;stat_map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hemi&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;bg_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sulcal_depth_map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bg_on_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;darkness&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_surf_stat_map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fsaverage5_pial&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stat_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;stat_map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hemi&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;medial&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bg_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sulcal_depth_map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;bg_on_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;darkness&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Displaying a thresholded stat map with a different colormap and transparency&lt;/span&gt;
&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_surf_stat_map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fsaverage5_pial&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stat_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;stat_map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hemi&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;bg_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sulcal_depth_map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bg_on_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Spectral&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="surface plot"></category><category term="atlas"></category><category term="statistical map"></category></entry></feed>